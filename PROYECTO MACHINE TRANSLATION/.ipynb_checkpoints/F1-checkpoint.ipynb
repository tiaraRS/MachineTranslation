{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e67de694",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "from numpy import array\n",
    "import tensorflow\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "#from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0edd0059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_sentences(filename):\n",
    "\treturn load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a78bc9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(lines): #assigns id to words in lines vocab\n",
    "\ttokenizer = Tokenizer() #default filters punctuation\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c8e221f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(dataset[:, 0][0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c986f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = ['hola que tal tal tal.', 'como estas hoy hola', 'mi perrito bello es','vamos a comer silpancho mas tarde']\n",
    "t = create_tokenizer(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff0c72c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document count 4\n",
      "The count of words 16\n",
      "The word index {'tal': 1, 'hola': 2, 'que': 3, 'como': 4, 'estas': 5, 'hoy': 6, 'mi': 7, 'perrito': 8, 'bello': 9, 'es': 10, 'vamos': 11, 'a': 12, 'comer': 13, 'silpancho': 14, 'mas': 15, 'tarde': 16}\n"
     ]
    }
   ],
   "source": [
    "print(\"The document count\",t.document_count)\n",
    "print(\"The count of words\",len(t.word_counts))\n",
    "print(\"The word index\",t.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73077b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(lines): #max words in a sentence\n",
    "\treturn max(len(line.split()) for line in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21e734f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b57e1602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sequences(tokenizer, length, lines): #returns dim (#lines,length)\n",
    "\t# integer encode sequences\n",
    "\tX = tokenizer.texts_to_sequences(lines)\n",
    "\t#creates id array from tokenizer ids for each sentence\n",
    "\t# pad sequences with 0 values\n",
    "\tX = pad_sequences(X, maxlen=length, padding='post') # adds 0s to the end(post) of sequence\n",
    "\treturn X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0b30a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 10)\n",
      "[[ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 2  4  5  0  0  0  0  0  0  0]\n",
      " [11  0  0  0  0  0  0  0  0  0]\n",
      " [ 3  1  5  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "ba=encode_sequences(t,10, [\"asd sdjid\",\"hola como estas\", \"chau vamos\", \"que tal estas\"])\n",
    "print(ba.shape)\n",
    "print(ba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29c74d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]], shape=(4, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#to_categorical : builds one-hot encoding representation for input array\n",
    "a = tensorflow.keras.utils.to_categorical([0, 1, 2, 3], num_classes=4)\n",
    "a = tensorflow.constant(a, shape=[4, 4])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ca7e790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_output(sequences, vocab_size): #returns 3d\n",
    "\tylist = np.array([])\n",
    "\tfor sequence in sequences:\n",
    "\t\tencoded = to_categorical(sequence, num_classes=vocab_size,dtype=int)\n",
    "\t\tylist=np.append(ylist,encoded)\n",
    "\ty = array(ylist)\n",
    "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "\treturn y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7805f6e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_output(encode_sequences(t,10,[\"fine thanks\",\"hola como estas\",\"hola que como tal como\"]), 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac7ac2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#none = batch dimension\n",
    "# batch size = # of samples in each batch during testing/training\n",
    "# timestemps = # of values in a sequence -> max # of words in sentences\n",
    "# features = # of dimensions to represent data\n",
    "#\t\t\t\t\t3856      2404       10            5              256\t\t\t\t\t\t\t\n",
    "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "\tmodel = Sequential()\n",
    "\t#src_vocab shape = (#german_sent, max#words in germ sentence)\n",
    "\t#src_vocab shape = (3856, 10)\n",
    "\t#src_vocab = SIZE  = 3856\n",
    "\n",
    "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True)) \n",
    "\t# 3856, 256 units outp, 10, \n",
    "\t# mask_zero -> tells model 0 is a padding number and cannot be used as index for vocabulary\n",
    "\t# input dim = vocab size + 1\n",
    "\n",
    "\t#EMBEDDING LAYER : \n",
    "\t# \tinput(batch_size, input_length) \n",
    "\t# \t->output (batch_size, input_length, output_dim)\n",
    "\t# (,10,256) - params = 3856x256\n",
    "\n",
    "\t#model.add(SimpleRNN(n_units,activation='relu'))\n",
    "\t#model.add(SimpleRNN())\n",
    "\t\n",
    "\n",
    "\tmodel.add(LSTM(n_units))\n",
    "\t#input 3D [batch_size, timesteps, feature] -> (,10,256)\n",
    "\t# output ,256 - params = \n",
    "\n",
    "\tmodel.add(RepeatVector(tar_timesteps))\n",
    "\t# repeats input n times\n",
    "\t# input = (batch_size, input_length, output_dim)\n",
    "\t# tar_timesteps = english max word # in a sentence\n",
    "\t# output = (batch_size, tar_timesteps, output_dim)\n",
    "\n",
    "\t#model.add(SimpleRNN(n_units, return_sequences=True))\n",
    "\tmodel.add(LSTM(n_units, return_sequences=True))#default sigmoid activation\n",
    "\t# return_sequence = boolean , return full sequence if true, else return just output sequence\n",
    "\t# input (batch_size, input_length, c)\n",
    "\t# output \n",
    "\t# \treturn_seq TRUE:(batch_size, input_length, 256) - (,5,256)\n",
    "\t#\treturn_seq FALSE:(batch_size,256)\n",
    "\n",
    "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "\t# input : (batch_size, input_length, 256)\n",
    "\t# output: (batch, batch_size, input_length, 256)\n",
    "\t# outputs should have same function for every timestep, we dont want flattened output\n",
    "\t# softmax returns probability array\n",
    "\t# Dense output: tar_vocab legnth array of probabilities for each word\n",
    "\t# 256x2404 + 2404(biases) = 618828\n",
    "\n",
    "\t#model.add(Dense(tar_vocab, activation='softmax'))\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0679787",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRUEBITA\n",
    "# from numpy import array\n",
    "# import numpy as np\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, TimeDistributed, LSTM\n",
    "# Input_Dim, Output_Dim = 15, 8\n",
    "# Length = 64\n",
    "# Sample_Size = 50\n",
    "# X = np.random.random([Sample_Size,Length,Input_Dim]) #(50,64,15)\n",
    "# y = np.random.random([Sample_Size,Length,Output_Dim]) #(50,64,8)\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(32, input_shape=(64, 15), return_sequences=True)) #(10,64,32)\n",
    "# model.add(TimeDistributed(Dense(8))) #(10,64,8)\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# print(X.shape, y.shape)\n",
    "# print(model.summary())\n",
    "# model.fit(X, y, epochs=100)\n",
    "# result = model.predict(X, batch_size=10, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d600d75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape inputs (32, 10, 8)\n"
     ]
    }
   ],
   "source": [
    "inputs = tensorflow.random.normal([32, 10, 8])\n",
    "print(\"shape inputs\", inputs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e833e9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(ds_filename, train_ds_fn, test_ds_fn):    \n",
    "    dataset = load_clean_sentences(ds_filename)\n",
    "    train = load_clean_sentences(train_ds_fn)\n",
    "    test = load_clean_sentences(test_ds_fn)\n",
    "    return dataset,train,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ba2be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_tokenizer(dataset):\n",
    "    tokenizer = create_tokenizer(dataset[:, 0])\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    max_sentence_length = max_length(dataset[:, 0])\n",
    "    return tokenizer,vocab_size,max_sentence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abcb831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input(origin_tok, origin_max_sent_length, target_tok, target_max_sent_length,target_vocab_size, data, one_hot=False):\n",
    "    dataX = encode_sequences(origin_tok, origin_max_sent_length, data[:, 1])\n",
    "    dataY = encode_sequences(target_tok, target_max_sent_length, data[:, 0])\n",
    "    if one_hot:\n",
    "        dataY = encode_output(dataY, target_vocab_size)\n",
    "    return dataX,dataY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fcc9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def graph_loss_vs_epochs(history):\n",
    "    training_loss = history.history['loss']\n",
    "    test_loss = history.history['val_loss']\n",
    "\n",
    "    # Create count of the number of epochs\n",
    "    epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "    # Visualize loss history\n",
    "    plt.plot(epoch_count, training_loss, 'r--')\n",
    "    plt.plot(epoch_count, test_loss, 'b-')\n",
    "    plt.legend(['Training Loss', 'Test Loss'])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9635251c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_model(trainX, trainY, testX,testY, epochs, batch_size, model, model_save_file_name):\n",
    "    checkpoint = ModelCheckpoint(model_save_file_name, monitor='val_loss', verbose=1,save_best_only=True, mode='min')\n",
    "    model.fit(trainX, trainY, epochs=epochs, batch_size=batch_size,  validation_data=(testX, testY))\n",
    "    history=model.history\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be7b236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model, units,model_sum_im_name,loss_func='categorical_crossentropy',learning_rate=0.001):\n",
    "    optimizer = keras.optimizers.Adam(learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=loss_func,metrics=['acc'])\n",
    "    #categorical cross entropy -> one hot encoding output\n",
    "    #sparse categorical cross entropy -> output as integers\n",
    "    # summarize defined model\n",
    "    print(model.summary())\n",
    "    plot_model(model, to_file=model_sum_im_name, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7cf3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset,train,testset=load_data(ds_filename, train_ds_fn, test_ds_fn)\n",
    "tokenizer,vocab_size,max_sentence_length = prepare_tokenizer(dataset)\n",
    "model = define_model(origin_vocab_size, target_vocab_size, origin_max_sent_length, target_max_sent_length, units)\n",
    "create_model(model, units,loss_func='categorical_crossentropy',learning_rate=0.001)\n",
    "train_evaluate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "612c570d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "dataset = load_clean_sentences('english-spanish-both.txt')\n",
    "train = load_clean_sentences('english-spanish-train.txt')\n",
    "test = load_clean_sentences('english-spanish-test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97e8872c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 2) (2000, 2) (117788, 2)\n",
      "[['fine today' 'hoy hace bueno']\n",
      " ['can we go' 'podemos ir']\n",
      " ['you were busy' 'estabas ocupada']\n",
      " ['can you whistle' 'podes silbar']\n",
      " ['tom speak' 'tom no puede hablar']\n",
      " ['it like' 'como es']\n",
      " ['french' 'soy frances']\n",
      " ['i like them all' 'me gustan todos']\n",
      " ['it was my fault' 'fue mi culpa']\n",
      " ['who sells this' 'quien vende esto']\n",
      " ['is it a trap' 'acaso es una trampa']\n",
      " ['on a diet' 'estoy a dieta']\n",
      " ['new' 'estan nuevas']\n",
      " ['tom might faint' 'tom quizas podria desfallecer']\n",
      " ['tom saw me' 'tom me vio']\n",
      " ['how was boston' 'que tal en boston']\n",
      " ['how do you feel' 'como os sentis']\n",
      " ['do you trust me' 'confias en mi']\n",
      " ['go get it' 'vete a por ello']\n",
      " ['do it again' 'lo volveria a hacer']\n",
      " ['what a jerk' 'que pendejo']\n",
      " ['tom did not cry' 'tom no lloro']\n",
      " ['a book' 'eso es un libro']\n",
      " ['this is a pen' 'esto es un boligrafo']\n",
      " ['know' 'lo sabran']\n",
      " ['dad home' 'papa no esta en casa']\n",
      " ['i need ice' 'necesito hielo']\n",
      " ['i live in hyogo' 'yo vivo en hyogo']\n",
      " ['mine' 'es mia']\n",
      " ['rather walk' 'preferiria caminar']\n",
      " ['she kissed him' 'ella le beso']\n",
      " ['she trusted you' 'ella confiaba en ti']\n",
      " ['undressing' 'me estoy desnudando']\n",
      " ['excited' 'estoy emocionado']\n",
      " ['listen' 'escuche']\n",
      " ['can i help you' 'te puedo ayudar']\n",
      " ['the birds sang' 'los pajaros cantaban']\n",
      " ['with me' 'ellas estan conmigo']\n",
      " ['continue working' 'seguid trabajando']\n",
      " ['you may go' 'podeis iros']\n",
      " ['give me half' 'dame la mitad']\n",
      " ['go up' 'vamos para arriba']\n",
      " ['i need a weapon' 'necesito un arma']\n",
      " ['now sad' 'ahora estoy triste']\n",
      " ['i want a lawyer' 'quiero un abogado']\n",
      " ['useless' 'es inutil']\n",
      " ['do you play golf' 'juegas golf']\n",
      " ['they found out' 'ellos lo descubrieron']\n",
      " ['he mentioned it' 'el lo menciono']\n",
      " ['cherries are red' 'las cerezas son rojas']\n",
      " ['i am curious' 'tengo curiosidad']\n",
      " ['not happy' 'no soy feliz']\n",
      " ['go away' 'largate']\n",
      " ['i need them' 'los necesito']\n",
      " ['remove your hat' 'quitate el sombrero']\n",
      " ['can you see it' 'puedes verlo']\n",
      " ['stand back' 'retrocede']\n",
      " ['tom escaped' 'tom escapo']\n",
      " ['just follow me' 'tan solo sigueme']\n",
      " ['try me' 'no me tientes']\n",
      " ['i met a friend' 'me encontre con una amiga']\n",
      " ['go by car' 'ire en coche']\n",
      " ['not perfect' 'no soy perfecta']\n",
      " ['i use firefox' 'yo uso firefox']\n",
      " ['did you know tom' 'conocias a tom']\n",
      " ['a good cook' 'se me da bien la cocina']\n",
      " ['turn on the tv' 'enciende la tele']\n",
      " ['no singer' 'ella no es ninguna cantante']\n",
      " ['tom loves dogs' 'tom ama a los perros']\n",
      " ['just do that' 'tan solo hazlo']\n",
      " ['i felt reborn' 'yo me senti renacido']\n",
      " ['seize him' 'a por el']\n",
      " ['cook for me' 'cociname']\n",
      " ['eat here' 'comere aqui']\n",
      " ['is anybody hurt' 'se lastimo alguien']\n",
      " ['i had doubts' 'yo tenia dudas']\n",
      " ['tom waved' 'tom saludo']\n",
      " ['come on' 'orale']\n",
      " ['they loved you' 'te querian']\n",
      " ['tom now' 'donde esta tom ahora']\n",
      " ['life easy' 'la vida no es facil']\n",
      " ['where is he' 'donde esta el']\n",
      " ['i made a deal' 'yo hice un trato']\n",
      " ['what a phony' 'que farsante']\n",
      " ['let me in' 'dejeme entrar']\n",
      " ['i saw him jump' 'le vi saltar']\n",
      " ['reward you' 'te premiare']\n",
      " ['an adult' 'soy adulta']\n",
      " ['he is very fast' 'el es muy rapido']\n",
      " ['please hold on' 'por favor espere']\n",
      " ['just look at me' 'mirame']\n",
      " ['i need time' 'necesito tiempo']\n",
      " ['i am studying' 'estoy estudiando']\n",
      " ['tom shot mary' 'tom le disparo a mary']\n",
      " ['we failed' 'hemos fallado']\n",
      " ['my treat' 'es de parte mia']\n",
      " ['tom is at work' 'tom esta en el trabajo']\n",
      " ['i need a car' 'necesito un coche']\n",
      " ['not a hero' 'no soy un heroe']\n",
      " ['please sing' 'canta por favor']]\n"
     ]
    }
   ],
   "source": [
    "print(train.shape, test.shape, dataset.shape)\n",
    "print(train[100:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85444ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 12715\n",
      "English Max Length: 47\n"
     ]
    }
   ],
   "source": [
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "print('English Max Length: %d' % (eng_length))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c0089be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spanish Vocabulary Size: 24581\n",
      "Spanish Max Length: 49\n"
     ]
    }
   ],
   "source": [
    "# prepare german tokenizer\n",
    "spa_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "spa_vocab_size = len(spa_tokenizer.word_index) + 1\n",
    "spa_length = max_length(dataset[:, 1])\n",
    "print('Spanish Vocabulary Size: %d' % spa_vocab_size)\n",
    "print('Spanish Max Length: %d' % (spa_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69ab42ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14372/1743753787.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtrainX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencode_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspa_tokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspa_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtrainY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencode_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meng_tokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meng_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtrainY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencode_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meng_vocab_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;31m# x son secuencias encoded,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# y son one hot encoded\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14372/3748153467.py\u001b[0m in \u001b[0;36mencode_output\u001b[1;34m(sequences, vocab_size)\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0msequence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msequences\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                 \u001b[0mencoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m                 \u001b[0mylist\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mylist\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mylist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msequences\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mappend\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36mappend\u001b[1;34m(arr, values, axis)\u001b[0m\n\u001b[0;32m   4743\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4744\u001b[0m         \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4745\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4746\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4747\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# prepare training data\n",
    "\n",
    "trainX = encode_sequences(spa_tokenizer, spa_length, train[:, 1])\n",
    "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
    "trainY = encode_output(trainY, eng_vocab_size)\n",
    "# x son secuencias encoded,\n",
    "# y son one hot encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd28bb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare validation data\n",
    "testX = encode_sequences(spa_tokenizer, spa_length, test[:, 1])\n",
    "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
    "testY = encode_output(testY, eng_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7145af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#usa las primeras 1000 oraciones\n",
    "# define model\n",
    "learning_rate=0.001\n",
    "model = define_model(spa_vocab_size, eng_vocab_size, spa_length, eng_length, 256)\n",
    "optimizer = keras.optimizers.Adam(learning_rate)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy',metrics=['acc'])\n",
    "#categorical cross entropy -> one hot encoding output\n",
    "#sparse categorical cross entropy -> output as integers\n",
    "# summarize defined model\n",
    "print(model.summary())\n",
    "#plot_model(model, to_file='model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ed7b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "#filename = 'model.h5'\n",
    "#checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "model.fit(trainX, trainY, epochs=30, batch_size=64,  validation_data=(testX, testY))#, callbacks=[checkpoint])#, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbf89c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X, Y, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    " \n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    " \n",
    "# later...\n",
    " \n",
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(X, Y, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53449a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import argmax\n",
    "# asignar un n√∫mero entero a una palabra\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    " \n",
    "# generar la secuencia de origen del objetivo\n",
    "def predict_sequence(model, tokenizer, source):\n",
    "    #print(f\"S {source}\")\n",
    "    prediction = model.predict(source, verbose=0)[0]\n",
    "    #print(f\"PREDICTION {prediction}\")\n",
    "    integers = [argmax(vector) for vector in prediction]\n",
    "    target = list()\n",
    "    for i in integers:\n",
    "        word = word_for_id(i, tokenizer)\n",
    "        #print(f\"WORD{word}\")\n",
    "        if word is None:\n",
    "            break\n",
    "        target.append(word)\n",
    "    return' '.join(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a10f635",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_prediction_test=encode_sequences(spa_tokenizer, spa_length, ['Hola como estas?'])\n",
    "print(x_prediction_test)\n",
    "print(x_prediction_test[0])\n",
    "predict_sequence(model,eng_tokenizer,x_prediction_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
