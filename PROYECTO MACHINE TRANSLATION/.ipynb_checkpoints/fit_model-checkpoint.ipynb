{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "from numpy import array\n",
    "import tensorflow\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "#from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_sentences(filename):\n",
    "\treturn load(open(filename, 'rb'))\n",
    "\n",
    "def create_tokenizer(lines): #assigns id to words in lines vocab\n",
    "\ttokenizer = Tokenizer() #default filters punctuation\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer\n",
    "\n",
    "def max_length(lines): #max words in a sentence\n",
    "\treturn max(len(line.split()) for line in lines)\n",
    "\n",
    "def encode_sequences(tokenizer, length, lines): #returns dim (#lines,length)\n",
    "\t# integer encode sequences\n",
    "\tX = tokenizer.texts_to_sequences(lines)\n",
    "\t#creates id array from tokenizer ids for each sentence\n",
    "\t# pad sequences with 0 values\n",
    "\tX = pad_sequences(X, maxlen=length, padding='post') # adds 0s to the end(post) of sequence\n",
    "\treturn X\n",
    "\n",
    "def encode_output(sequences, vocab_size): #returns 3d\n",
    "\tylist = np.array([])\n",
    "\tfor sequence in sequences:\n",
    "\t\tencoded = to_categorical(sequence, num_classes=vocab_size,dtype=int)\n",
    "\t\tylist=np.append(ylist,encoded)\n",
    "\ty = array(ylist)\n",
    "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "\treturn y\n",
    "\n",
    "def load_data(ds_filename, train_ds_fn, test_ds_fn):    \n",
    "    dataset = load_clean_sentences(ds_filename)\n",
    "    train = load_clean_sentences(train_ds_fn)\n",
    "    test = load_clean_sentences(test_ds_fn)\n",
    "    return dataset,train,test\n",
    "\n",
    "def prepare_tokenizer(dataset, index):\n",
    "    tokenizer = create_tokenizer(dataset[:, index])\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    max_sentence_length = max_length(dataset[:, index])\n",
    "    return tokenizer,vocab_size,max_sentence_length\n",
    "\n",
    "def preprocess_input(origin_tok, origin_max_sent_length, target_tok, target_max_sent_length,target_vocab_size, data, one_hot=False):\n",
    "    dataX = encode_sequences(origin_tok, origin_max_sent_length, data[:, 0])\n",
    "    dataY = encode_sequences(target_tok, target_max_sent_length, data[:, 1])\n",
    "    if one_hot:\n",
    "        dataY = encode_output(dataY, target_vocab_size)\n",
    "    return dataX,dataY\n",
    "\n",
    "def graph_loss_vs_epochs(history, save_image_filename, title):\n",
    "    training_loss = history.history['loss']\n",
    "    test_loss = history.history['val_loss'] #[10 9 8 5 6 7] 3\n",
    "\n",
    "    # Create count of the number of epochs\n",
    "    epoch_count = range(1, len(training_loss) + 1) #[1 2 3 4 5 6]\n",
    "\n",
    "    # Visualize loss history\n",
    "    plt.title(title)\n",
    "    plt.plot(epoch_count, training_loss, 'r--')\n",
    "    plt.plot(epoch_count, test_loss, 'b-')\n",
    "    plt.legend(['Training Loss', 'Test Loss'])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.savefig(save_image_filename) \n",
    "    plt.axvline(x = epoch_count[test_loss.index(min(test_loss))], color = 'c', linestyle=\"dotted\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_model(trainX, trainY, testX,testY, epochs, batch_size, model, model_save_file_name):\n",
    "    checkpoint = ModelCheckpoint(model_save_file_name, monitor='val_loss', verbose=1,save_best_only=True, mode='min')\n",
    "    model.fit(trainX, trainY, epochs=epochs, batch_size=batch_size,  validation_data=(testX, testY),callbacks=[checkpoint], verbose=2)\n",
    "\n",
    "def create_model(model,loss_func='categorical_crossentropy',learning_rate=0.001):\n",
    "    optimizer = Adam(learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=loss_func,metrics=['acc'])\n",
    "    #categorical cross entropy -> one hot encoding output\n",
    "    #sparse categorical cross entropy -> output as integers\n",
    "    # summarize defined model\n",
    "    print(model.summary())\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True)) \n",
    "\tmodel.add(LSTM(n_units))\n",
    "\tmodel.add(RepeatVector(tar_timesteps))\n",
    "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
    "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "\treturn model\n",
    "\n",
    "def define_model_2(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True)) \n",
    "\tmodel.add(GRU(64, return_sequences=True))\n",
    "\t#model.add(RepeatVector(tar_timesteps))\n",
    "\t#model.add(LSTM(n_units, return_sequences=True))\n",
    "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2272 4510 5 8\n",
      "(8000, 5) (8000, 8) (2000, 5) (2000, 8)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 5, 256)            581632    \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 5, 64)             61824     \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 5, 4510)          293150    \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 936,606\n",
      "Trainable params: 936,606\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\Camil\\anaconda3\\envs\\SistemasInteligentes\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Camil\\anaconda3\\envs\\SistemasInteligentes\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Camil\\anaconda3\\envs\\SistemasInteligentes\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Camil\\anaconda3\\envs\\SistemasInteligentes\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\Camil\\anaconda3\\envs\\SistemasInteligentes\\lib\\site-packages\\keras\\engine\\training.py\", line 918, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\Camil\\anaconda3\\envs\\SistemasInteligentes\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\Camil\\anaconda3\\envs\\SistemasInteligentes\\lib\\site-packages\\keras\\losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\Camil\\anaconda3\\envs\\SistemasInteligentes\\lib\\site-packages\\keras\\losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\Camil\\anaconda3\\envs\\SistemasInteligentes\\lib\\site-packages\\keras\\losses.py\", line 1862, in sparse_categorical_crossentropy\n        return backend.sparse_categorical_crossentropy(\n    File \"C:\\Users\\Camil\\anaconda3\\envs\\SistemasInteligentes\\lib\\site-packages\\keras\\backend.py\", line 5202, in sparse_categorical_crossentropy\n        res = tf.nn.sparse_softmax_cross_entropy_with_logits(\n\n    ValueError: `labels.shape` must equal `logits.shape` except for the last dimension. Received: labels.shape=(64, 8) and logits.shape=(64, 5, 4510)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Camil\\Documents\\Ingeniería de Sistemas - UCB\\I - 2022\\Sistemas Inteligentes\\Practicas\\Cuarta - MachineTranslation\\MachineTranslation\\PROYECTO MACHINE TRANSLATION\\fit_model.ipynb Cell 5'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Camil/Documents/Ingenier%C3%ADa%20de%20Sistemas%20-%20UCB/I%20-%202022/Sistemas%20Inteligentes/Practicas/Cuarta%20-%20MachineTranslation/MachineTranslation/PROYECTO%20MACHINE%20TRANSLATION/fit_model.ipynb#ch0000004?line=19'>20</a>\u001b[0m create_model(model,loss_func,learning_rate)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Camil/Documents/Ingenier%C3%ADa%20de%20Sistemas%20-%20UCB/I%20-%202022/Sistemas%20Inteligentes/Practicas/Cuarta%20-%20MachineTranslation/MachineTranslation/PROYECTO%20MACHINE%20TRANSLATION/fit_model.ipynb#ch0000004?line=20'>21</a>\u001b[0m plot_model(model, to_file\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmodel_images/im5test.png\u001b[39m\u001b[39m'\u001b[39m, show_shapes\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Camil/Documents/Ingenier%C3%ADa%20de%20Sistemas%20-%20UCB/I%20-%202022/Sistemas%20Inteligentes/Practicas/Cuarta%20-%20MachineTranslation/MachineTranslation/PROYECTO%20MACHINE%20TRANSLATION/fit_model.ipynb#ch0000004?line=21'>22</a>\u001b[0m train_evaluate_model(trainX, trainY, testX,testY, epochs, batch_size, model, model_save_file_name)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Camil/Documents/Ingenier%C3%ADa%20de%20Sistemas%20-%20UCB/I%20-%202022/Sistemas%20Inteligentes/Practicas/Cuarta%20-%20MachineTranslation/MachineTranslation/PROYECTO%20MACHINE%20TRANSLATION/fit_model.ipynb#ch0000004?line=22'>23</a>\u001b[0m graph_loss_vs_epochs(model\u001b[39m.\u001b[39mhistory, \u001b[39m'\u001b[39m\u001b[39mloss_vs_epochs_images/im5test.png\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtest5\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\Camil\\Documents\\Ingeniería de Sistemas - UCB\\I - 2022\\Sistemas Inteligentes\\Practicas\\Cuarta - MachineTranslation\\MachineTranslation\\PROYECTO MACHINE TRANSLATION\\fit_model.ipynb Cell 3'\u001b[0m in \u001b[0;36mtrain_evaluate_model\u001b[1;34m(trainX, trainY, testX, testY, epochs, batch_size, model, model_save_file_name)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Camil/Documents/Ingenier%C3%ADa%20de%20Sistemas%20-%20UCB/I%20-%202022/Sistemas%20Inteligentes/Practicas/Cuarta%20-%20MachineTranslation/MachineTranslation/PROYECTO%20MACHINE%20TRANSLATION/fit_model.ipynb#ch0000002?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_evaluate_model\u001b[39m(trainX, trainY, testX,testY, epochs, batch_size, model, model_save_file_name):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Camil/Documents/Ingenier%C3%ADa%20de%20Sistemas%20-%20UCB/I%20-%202022/Sistemas%20Inteligentes/Practicas/Cuarta%20-%20MachineTranslation/MachineTranslation/PROYECTO%20MACHINE%20TRANSLATION/fit_model.ipynb#ch0000002?line=1'>2</a>\u001b[0m     checkpoint \u001b[39m=\u001b[39m ModelCheckpoint(model_save_file_name, monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,save_best_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Camil/Documents/Ingenier%C3%ADa%20de%20Sistemas%20-%20UCB/I%20-%202022/Sistemas%20Inteligentes/Practicas/Cuarta%20-%20MachineTranslation/MachineTranslation/PROYECTO%20MACHINE%20TRANSLATION/fit_model.ipynb#ch0000002?line=2'>3</a>\u001b[0m     model\u001b[39m.\u001b[39;49mfit(trainX, trainY, epochs\u001b[39m=\u001b[39;49mepochs, batch_size\u001b[39m=\u001b[39;49mbatch_size,  validation_data\u001b[39m=\u001b[39;49m(testX, testY),callbacks\u001b[39m=\u001b[39;49m[checkpoint], verbose\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\SistemasInteligentes\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Camil/anaconda3/envs/SistemasInteligentes/lib/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/Camil/anaconda3/envs/SistemasInteligentes/lib/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> <a href='file:///c%3A/Users/Camil/anaconda3/envs/SistemasInteligentes/lib/site-packages/keras/utils/traceback_utils.py?line=66'>67</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     <a href='file:///c%3A/Users/Camil/anaconda3/envs/SistemasInteligentes/lib/site-packages/keras/utils/traceback_utils.py?line=67'>68</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/Camil/anaconda3/envs/SistemasInteligentes/lib/site-packages/keras/utils/traceback_utils.py?line=68'>69</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\SistemasInteligentes\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1147\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Camil/anaconda3/envs/SistemasInteligentes/lib/site-packages/tensorflow/python/framework/func_graph.py?line=1144'>1145</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Camil/anaconda3/envs/SistemasInteligentes/lib/site-packages/tensorflow/python/framework/func_graph.py?line=1145'>1146</a>\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> <a href='file:///c%3A/Users/Camil/anaconda3/envs/SistemasInteligentes/lib/site-packages/tensorflow/python/framework/func_graph.py?line=1146'>1147</a>\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mag_error_metadata\u001b[39m.\u001b[39mto_exception(e)\n\u001b[0;32m   <a href='file:///c%3A/Users/Camil/anaconda3/envs/SistemasInteligentes/lib/site-packages/tensorflow/python/framework/func_graph.py?line=1147'>1148</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/Camil/anaconda3/envs/SistemasInteligentes/lib/site-packages/tensorflow/python/framework/func_graph.py?line=1148'>1149</a>\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\Camil\\anaconda3\\envs\\SistemasInteligentes\\lib\\site-packages\\keras\\engine\\training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Camil\\anaconda3\\envs\\SistemasInteligentes\\lib\\site-packages\\keras\\engine\\training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Camil\\anaconda3\\envs\\SistemasInteligentes\\lib\\site-packages\\keras\\engine\\training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Camil\\anaconda3\\envs\\SistemasInteligentes\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\Camil\\anaconda3\\envs\\SistemasInteligentes\\lib\\site-packages\\keras\\engine\\training.py\", line 918, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\Camil\\anaconda3\\envs\\SistemasInteligentes\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\Camil\\anaconda3\\envs\\SistemasInteligentes\\lib\\site-packages\\keras\\losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\Camil\\anaconda3\\envs\\SistemasInteligentes\\lib\\site-packages\\keras\\losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\Camil\\anaconda3\\envs\\SistemasInteligentes\\lib\\site-packages\\keras\\losses.py\", line 1862, in sparse_categorical_crossentropy\n        return backend.sparse_categorical_crossentropy(\n    File \"C:\\Users\\Camil\\anaconda3\\envs\\SistemasInteligentes\\lib\\site-packages\\keras\\backend.py\", line 5202, in sparse_categorical_crossentropy\n        res = tf.nn.sparse_softmax_cross_entropy_with_logits(\n\n    ValueError: `labels.shape` must equal `logits.shape` except for the last dimension. Received: labels.shape=(64, 8) and logits.shape=(64, 5, 4510)\n"
     ]
    }
   ],
   "source": [
    "ds_filename, train_ds_fn, test_ds_fn = 'dataset/english-spanish-both-10000.txt', 'dataset/english-spanish-train-10000.txt','dataset/english-spanish-test-10000.txt'\n",
    "units = 256\n",
    "learning_rate = 0.001\n",
    "loss_func='categorical_crossentropy'\n",
    "epochs=3\n",
    "batch_size=64\n",
    "model_save_file_name='Models/model_test5.h5'\n",
    "\n",
    "dataset,train,test=load_data(ds_filename, train_ds_fn, test_ds_fn)\n",
    "\n",
    "eng_tokenizer,eng_vocab_size,eng_max_sentence_length = prepare_tokenizer(dataset,0)\n",
    "spa_tokenizer,spa_vocab_size,spa_max_sentence_length = prepare_tokenizer(dataset,1)\n",
    "print(eng_vocab_size,spa_vocab_size,eng_max_sentence_length,spa_max_sentence_length)\n",
    "\n",
    "trainX, trainY =  preprocess_input(eng_tokenizer, eng_max_sentence_length, spa_tokenizer, spa_max_sentence_length,spa_vocab_size, train, one_hot=False)\n",
    "testX, testY = preprocess_input(eng_tokenizer, eng_max_sentence_length, spa_tokenizer, spa_max_sentence_length,spa_vocab_size, test, one_hot=False)\n",
    "print(trainX.shape, trainY.shape, testX.shape, testY.shape)\n",
    "\n",
    "model = define_model_2(eng_vocab_size, spa_vocab_size, eng_max_sentence_length, spa_max_sentence_length, units)\n",
    "create_model(model,loss_func,learning_rate)\n",
    "plot_model(model, to_file='model_images/im5test.png', show_shapes=True)\n",
    "train_evaluate_model(trainX, trainY, testX,testY, epochs, batch_size, model, model_save_file_name)\n",
    "graph_loss_vs_epochs(model.history, 'loss_vs_epochs_images/im5test.png', 'test5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aba33f5906df6e7b2943b50860cc645261c57a13f9a1e1d4acadf1f03aacb37d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
