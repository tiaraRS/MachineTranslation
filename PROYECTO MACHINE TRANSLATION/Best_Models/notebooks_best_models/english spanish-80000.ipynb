{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5019a158",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fit_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9b6f23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "from pickle import dump\n",
    "from numpy.random import rand\n",
    "from numpy.random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ff10765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117788\n",
      "80000\n",
      "64000 16000\n",
      "Saved: dataset/english-spanish-both-VF-80000.txt\n",
      "Saved: dataset/english-spanish-train-VF-80000.txt\n",
      "Saved: dataset/english-spanish-test-VF-80000.txt\n"
     ]
    }
   ],
   "source": [
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "\treturn load(open(filename, 'rb'))\n",
    "\n",
    "# save a list of clean sentences to file\n",
    "def save_clean_data(sentences, filename):\n",
    "\tdump(sentences, open(filename, 'wb'))\n",
    "\tprint('Saved: %s' % filename)\n",
    "\n",
    "# load dataset\n",
    "raw_dataset = load_clean_sentences('spanish-english-clean-3.txt')\n",
    "dataset_number_of_samples = raw_dataset.shape[0]\n",
    "print(dataset_number_of_samples)\n",
    "\n",
    "# reduce dataset size\n",
    "n_sentences = 80000\n",
    "\n",
    "# random shuffle\n",
    "shuffle(raw_dataset)\n",
    "\n",
    "dataset = raw_dataset[:n_sentences, :]\n",
    "\n",
    "# split into train/test\n",
    "print(dataset.shape[0])\n",
    "\n",
    "index_80=int(dataset.shape[0]*0.8)\n",
    "train, test = dataset[:index_80], dataset[index_80:]\n",
    "print(len(train),len(test))\n",
    "# save\n",
    "save_clean_data(dataset, 'dataset/english-spanish-both-VF-80000.txt')\n",
    "save_clean_data(train, 'dataset/english-spanish-train-VF-80000.txt')\n",
    "save_clean_data(test, 'dataset/english-spanish-test-VF-80000.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2503e1b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11234 20955 47 49\n",
      "(64000, 47) (64000, 49) (16000, 47) (16000, 49)\n",
      "(64000, 47) (64000, 49) (16000, 47) (16000, 49)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 47, 128)           1437952   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 256)              198144    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " repeat_vector (RepeatVector  (None, 49, 256)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 49, 256)          296448    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 49, 20955)        5385435   \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,317,979\n",
      "Trainable params: 7,317,979\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.87657, saving model to best_model-VF-80000.h5\n",
      "2000/2000 - 2592s - loss: 0.9755 - acc: 0.8793 - val_loss: 0.8766 - val_acc: 0.8805 - 2592s/epoch - 1s/step\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 2: val_loss improved from 0.87657 to 0.83248, saving model to best_model-VF-80000.h5\n",
      "2000/2000 - 2535s - loss: 0.8499 - acc: 0.8838 - val_loss: 0.8325 - val_acc: 0.8858 - 2535s/epoch - 1s/step\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 3: val_loss improved from 0.83248 to 0.77431, saving model to best_model-VF-80000.h5\n",
      "2000/2000 - 2534s - loss: 0.7876 - acc: 0.8885 - val_loss: 0.7743 - val_acc: 0.8897 - 2534s/epoch - 1s/step\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 4: val_loss improved from 0.77431 to 0.70297, saving model to best_model-VF-80000.h5\n",
      "2000/2000 - 2529s - loss: 0.7129 - acc: 0.8930 - val_loss: 0.7030 - val_acc: 0.8941 - 2529s/epoch - 1s/step\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 5: val_loss improved from 0.70297 to 0.63114, saving model to best_model-VF-80000.h5\n",
      "2000/2000 - 2532s - loss: 0.6203 - acc: 0.8986 - val_loss: 0.6311 - val_acc: 0.8993 - 2532s/epoch - 1s/step\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 6: val_loss improved from 0.63114 to 0.58486, saving model to best_model-VF-80000.h5\n",
      "2000/2000 - 2532s - loss: 0.5359 - acc: 0.9035 - val_loss: 0.5849 - val_acc: 0.9018 - 2532s/epoch - 1s/step\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 7: val_loss improved from 0.58486 to 0.55558, saving model to best_model-VF-80000.h5\n",
      "2000/2000 - 2535s - loss: 0.4703 - acc: 0.9078 - val_loss: 0.5556 - val_acc: 0.9042 - 2535s/epoch - 1s/step\n",
      "Epoch 8/100\n",
      "\n",
      "Epoch 8: val_loss improved from 0.55558 to 0.53971, saving model to best_model-VF-80000.h5\n",
      "2000/2000 - 2533s - loss: 0.4195 - acc: 0.9116 - val_loss: 0.5397 - val_acc: 0.9060 - 2533s/epoch - 1s/step\n",
      "Epoch 9/100\n",
      "\n",
      "Epoch 9: val_loss improved from 0.53971 to 0.52997, saving model to best_model-VF-80000.h5\n",
      "2000/2000 - 2534s - loss: 0.3798 - acc: 0.9154 - val_loss: 0.5300 - val_acc: 0.9065 - 2534s/epoch - 1s/step\n",
      "Epoch 10/100\n",
      "\n",
      "Epoch 10: val_loss improved from 0.52997 to 0.52720, saving model to best_model-VF-80000.h5\n",
      "2000/2000 - 2538s - loss: 0.3485 - acc: 0.9193 - val_loss: 0.5272 - val_acc: 0.9069 - 2538s/epoch - 1s/step\n",
      "Epoch 11/100\n",
      "\n",
      "Epoch 11: val_loss improved from 0.52720 to 0.52437, saving model to best_model-VF-80000.h5\n",
      "2000/2000 - 2539s - loss: 0.3250 - acc: 0.9227 - val_loss: 0.5244 - val_acc: 0.9077 - 2539s/epoch - 1s/step\n",
      "Epoch 12/100\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2539s - loss: 0.3062 - acc: 0.9254 - val_loss: 0.5246 - val_acc: 0.9078 - 2539s/epoch - 1s/step\n",
      "Epoch 13/100\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2542s - loss: 0.2897 - acc: 0.9279 - val_loss: 0.5293 - val_acc: 0.9090 - 2542s/epoch - 1s/step\n",
      "Epoch 14/100\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2538s - loss: 0.2756 - acc: 0.9304 - val_loss: 0.5299 - val_acc: 0.9084 - 2538s/epoch - 1s/step\n",
      "Epoch 15/100\n",
      "\n",
      "Epoch 15: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2541s - loss: 0.2629 - acc: 0.9327 - val_loss: 0.5369 - val_acc: 0.9096 - 2541s/epoch - 1s/step\n",
      "Epoch 16/100\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2541s - loss: 0.2518 - acc: 0.9348 - val_loss: 0.5405 - val_acc: 0.9084 - 2541s/epoch - 1s/step\n",
      "Epoch 17/100\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2559s - loss: 0.2418 - acc: 0.9368 - val_loss: 0.5457 - val_acc: 0.9093 - 2559s/epoch - 1s/step\n",
      "Epoch 18/100\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2544s - loss: 0.2323 - acc: 0.9389 - val_loss: 0.5489 - val_acc: 0.9089 - 2544s/epoch - 1s/step\n",
      "Epoch 19/100\n",
      "\n",
      "Epoch 19: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2535s - loss: 0.2241 - acc: 0.9405 - val_loss: 0.5555 - val_acc: 0.9089 - 2535s/epoch - 1s/step\n",
      "Epoch 20/100\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2538s - loss: 0.2162 - acc: 0.9423 - val_loss: 0.5608 - val_acc: 0.9082 - 2538s/epoch - 1s/step\n",
      "Epoch 21/100\n",
      "\n",
      "Epoch 21: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2537s - loss: 0.2093 - acc: 0.9439 - val_loss: 0.5677 - val_acc: 0.9077 - 2537s/epoch - 1s/step\n",
      "Epoch 22/100\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2541s - loss: 0.2029 - acc: 0.9452 - val_loss: 0.5723 - val_acc: 0.9082 - 2541s/epoch - 1s/step\n",
      "Epoch 23/100\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2535s - loss: 0.1966 - acc: 0.9467 - val_loss: 0.5786 - val_acc: 0.9081 - 2535s/epoch - 1s/step\n",
      "Epoch 24/100\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2546s - loss: 0.1912 - acc: 0.9479 - val_loss: 0.5847 - val_acc: 0.9082 - 2546s/epoch - 1s/step\n",
      "Epoch 25/100\n",
      "\n",
      "Epoch 25: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2549s - loss: 0.1866 - acc: 0.9490 - val_loss: 0.5899 - val_acc: 0.9076 - 2549s/epoch - 1s/step\n",
      "Epoch 26/100\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2550s - loss: 0.1816 - acc: 0.9502 - val_loss: 0.5963 - val_acc: 0.9075 - 2550s/epoch - 1s/step\n",
      "Epoch 27/100\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2548s - loss: 0.1777 - acc: 0.9512 - val_loss: 0.6012 - val_acc: 0.9078 - 2548s/epoch - 1s/step\n",
      "Epoch 28/100\n",
      "\n",
      "Epoch 28: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2551s - loss: 0.1730 - acc: 0.9522 - val_loss: 0.6058 - val_acc: 0.9073 - 2551s/epoch - 1s/step\n",
      "Epoch 29/100\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2544s - loss: 0.1695 - acc: 0.9533 - val_loss: 0.6114 - val_acc: 0.9074 - 2544s/epoch - 1s/step\n",
      "Epoch 30/100\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2557s - loss: 0.1667 - acc: 0.9537 - val_loss: 0.6193 - val_acc: 0.9079 - 2557s/epoch - 1s/step\n",
      "Epoch 31/100\n",
      "\n",
      "Epoch 31: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2554s - loss: 0.1630 - acc: 0.9547 - val_loss: 0.6214 - val_acc: 0.9071 - 2554s/epoch - 1s/step\n",
      "Epoch 32/100\n",
      "\n",
      "Epoch 32: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2555s - loss: 0.1598 - acc: 0.9553 - val_loss: 0.6279 - val_acc: 0.9075 - 2555s/epoch - 1s/step\n",
      "Epoch 33/100\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2563s - loss: 0.1569 - acc: 0.9561 - val_loss: 0.6322 - val_acc: 0.9078 - 2563s/epoch - 1s/step\n",
      "Epoch 34/100\n",
      "\n",
      "Epoch 34: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2563s - loss: 0.1541 - acc: 0.9569 - val_loss: 0.6368 - val_acc: 0.9056 - 2563s/epoch - 1s/step\n",
      "Epoch 35/100\n",
      "\n",
      "Epoch 35: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2541s - loss: 0.1523 - acc: 0.9573 - val_loss: 0.6427 - val_acc: 0.9076 - 2541s/epoch - 1s/step\n",
      "Epoch 36/100\n",
      "\n",
      "Epoch 36: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2573s - loss: 0.1490 - acc: 0.9581 - val_loss: 0.6462 - val_acc: 0.9080 - 2573s/epoch - 1s/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/100\n",
      "\n",
      "Epoch 37: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2762s - loss: 0.1475 - acc: 0.9584 - val_loss: 0.6491 - val_acc: 0.9072 - 2762s/epoch - 1s/step\n",
      "Epoch 38/100\n",
      "\n",
      "Epoch 38: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2831s - loss: 0.1447 - acc: 0.9591 - val_loss: 0.6548 - val_acc: 0.9067 - 2831s/epoch - 1s/step\n",
      "Epoch 39/100\n",
      "\n",
      "Epoch 39: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2834s - loss: 0.1428 - acc: 0.9596 - val_loss: 0.6569 - val_acc: 0.9067 - 2834s/epoch - 1s/step\n",
      "Epoch 40/100\n",
      "\n",
      "Epoch 40: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2951s - loss: 0.1446 - acc: 0.9591 - val_loss: 0.6600 - val_acc: 0.9063 - 2951s/epoch - 1s/step\n",
      "Epoch 41/100\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2827s - loss: 0.1379 - acc: 0.9609 - val_loss: 0.6680 - val_acc: 0.9059 - 2827s/epoch - 1s/step\n",
      "Epoch 42/100\n",
      "\n",
      "Epoch 42: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2753s - loss: 0.1372 - acc: 0.9611 - val_loss: 0.6691 - val_acc: 0.9063 - 2753s/epoch - 1s/step\n",
      "Epoch 43/100\n",
      "\n",
      "Epoch 43: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2856s - loss: 0.1363 - acc: 0.9613 - val_loss: 0.6730 - val_acc: 0.9066 - 2856s/epoch - 1s/step\n",
      "Epoch 44/100\n",
      "\n",
      "Epoch 44: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2869s - loss: 0.1340 - acc: 0.9618 - val_loss: 0.6752 - val_acc: 0.9059 - 2869s/epoch - 1s/step\n",
      "Epoch 45/100\n",
      "\n",
      "Epoch 45: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2778s - loss: 0.1332 - acc: 0.9621 - val_loss: 0.6786 - val_acc: 0.9067 - 2778s/epoch - 1s/step\n",
      "Epoch 46/100\n",
      "\n",
      "Epoch 46: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2772s - loss: 0.1313 - acc: 0.9627 - val_loss: 0.6843 - val_acc: 0.9067 - 2772s/epoch - 1s/step\n",
      "Epoch 47/100\n",
      "\n",
      "Epoch 47: val_loss did not improve from 0.52437\n",
      "2000/2000 - 3114s - loss: 0.1301 - acc: 0.9629 - val_loss: 0.6869 - val_acc: 0.9068 - 3114s/epoch - 2s/step\n",
      "Epoch 48/100\n",
      "\n",
      "Epoch 48: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2830s - loss: 0.1291 - acc: 0.9631 - val_loss: 0.6895 - val_acc: 0.9062 - 2830s/epoch - 1s/step\n",
      "Epoch 49/100\n",
      "\n",
      "Epoch 49: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2738s - loss: 0.1277 - acc: 0.9636 - val_loss: 0.6929 - val_acc: 0.9063 - 2738s/epoch - 1s/step\n",
      "Epoch 50/100\n",
      "\n",
      "Epoch 50: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2696s - loss: 0.1266 - acc: 0.9638 - val_loss: 0.6959 - val_acc: 0.9054 - 2696s/epoch - 1s/step\n",
      "Epoch 51/100\n",
      "\n",
      "Epoch 51: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2700s - loss: 0.1263 - acc: 0.9638 - val_loss: 0.6978 - val_acc: 0.9051 - 2700s/epoch - 1s/step\n",
      "Epoch 52/100\n",
      "\n",
      "Epoch 52: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2718s - loss: 0.1247 - acc: 0.9642 - val_loss: 0.7041 - val_acc: 0.9045 - 2718s/epoch - 1s/step\n",
      "Epoch 53/100\n",
      "\n",
      "Epoch 53: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2708s - loss: 0.1234 - acc: 0.9646 - val_loss: 0.7050 - val_acc: 0.9059 - 2708s/epoch - 1s/step\n",
      "Epoch 54/100\n",
      "\n",
      "Epoch 54: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2703s - loss: 0.1276 - acc: 0.9635 - val_loss: 0.7065 - val_acc: 0.9058 - 2703s/epoch - 1s/step\n",
      "Epoch 55/100\n",
      "\n",
      "Epoch 55: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2704s - loss: 0.1198 - acc: 0.9656 - val_loss: 0.7081 - val_acc: 0.9055 - 2704s/epoch - 1s/step\n",
      "Epoch 56/100\n",
      "\n",
      "Epoch 56: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2705s - loss: 0.1201 - acc: 0.9655 - val_loss: 0.7112 - val_acc: 0.9063 - 2705s/epoch - 1s/step\n",
      "Epoch 57/100\n",
      "\n",
      "Epoch 57: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2707s - loss: 0.1204 - acc: 0.9653 - val_loss: 0.7165 - val_acc: 0.9069 - 2707s/epoch - 1s/step\n",
      "Epoch 58/100\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2707s - loss: 0.1188 - acc: 0.9659 - val_loss: 0.7197 - val_acc: 0.9065 - 2707s/epoch - 1s/step\n",
      "Epoch 59/100\n",
      "\n",
      "Epoch 59: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2708s - loss: 0.1178 - acc: 0.9661 - val_loss: 0.7190 - val_acc: 0.9063 - 2708s/epoch - 1s/step\n",
      "Epoch 60/100\n",
      "\n",
      "Epoch 60: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2711s - loss: 0.1181 - acc: 0.9661 - val_loss: 0.7226 - val_acc: 0.9057 - 2711s/epoch - 1s/step\n",
      "Epoch 61/100\n",
      "\n",
      "Epoch 61: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2708s - loss: 0.1170 - acc: 0.9664 - val_loss: 0.7236 - val_acc: 0.9052 - 2708s/epoch - 1s/step\n",
      "Epoch 62/100\n",
      "\n",
      "Epoch 62: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2710s - loss: 0.1165 - acc: 0.9664 - val_loss: 0.7274 - val_acc: 0.9059 - 2710s/epoch - 1s/step\n",
      "Epoch 63/100\n",
      "\n",
      "Epoch 63: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2715s - loss: 0.1159 - acc: 0.9666 - val_loss: 0.7259 - val_acc: 0.9049 - 2715s/epoch - 1s/step\n",
      "Epoch 64/100\n",
      "\n",
      "Epoch 64: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2712s - loss: 0.1148 - acc: 0.9669 - val_loss: 0.7305 - val_acc: 0.9058 - 2712s/epoch - 1s/step\n",
      "Epoch 65/100\n",
      "\n",
      "Epoch 65: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2688s - loss: 0.1138 - acc: 0.9671 - val_loss: 0.7354 - val_acc: 0.9054 - 2688s/epoch - 1s/step\n",
      "Epoch 66/100\n",
      "\n",
      "Epoch 66: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2594s - loss: 0.1144 - acc: 0.9670 - val_loss: 0.7351 - val_acc: 0.9059 - 2594s/epoch - 1s/step\n",
      "Epoch 67/100\n",
      "\n",
      "Epoch 67: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2625s - loss: 0.1118 - acc: 0.9676 - val_loss: 0.7369 - val_acc: 0.9059 - 2625s/epoch - 1s/step\n",
      "Epoch 68/100\n",
      "\n",
      "Epoch 68: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2894s - loss: 0.1114 - acc: 0.9678 - val_loss: 0.7389 - val_acc: 0.9053 - 2894s/epoch - 1s/step\n",
      "Epoch 69/100\n",
      "\n",
      "Epoch 69: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2738s - loss: 0.1115 - acc: 0.9678 - val_loss: 0.7444 - val_acc: 0.9059 - 2738s/epoch - 1s/step\n",
      "Epoch 70/100\n",
      "\n",
      "Epoch 70: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2743s - loss: 0.1109 - acc: 0.9680 - val_loss: 0.7470 - val_acc: 0.9059 - 2743s/epoch - 1s/step\n",
      "Epoch 71/100\n",
      "\n",
      "Epoch 71: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2734s - loss: 0.1106 - acc: 0.9680 - val_loss: 0.7455 - val_acc: 0.9056 - 2734s/epoch - 1s/step\n",
      "Epoch 72/100\n",
      "\n",
      "Epoch 72: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2742s - loss: 0.1093 - acc: 0.9684 - val_loss: 0.7482 - val_acc: 0.9050 - 2742s/epoch - 1s/step\n",
      "Epoch 73/100\n",
      "\n",
      "Epoch 73: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2735s - loss: 0.1095 - acc: 0.9683 - val_loss: 0.7502 - val_acc: 0.9055 - 2735s/epoch - 1s/step\n",
      "Epoch 74/100\n",
      "\n",
      "Epoch 74: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2738s - loss: 0.1091 - acc: 0.9684 - val_loss: 0.7543 - val_acc: 0.9062 - 2738s/epoch - 1s/step\n",
      "Epoch 75/100\n",
      "\n",
      "Epoch 75: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2738s - loss: 0.1075 - acc: 0.9689 - val_loss: 0.7557 - val_acc: 0.9054 - 2738s/epoch - 1s/step\n",
      "Epoch 76/100\n",
      "\n",
      "Epoch 76: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2747s - loss: 0.1079 - acc: 0.9688 - val_loss: 0.7574 - val_acc: 0.9055 - 2747s/epoch - 1s/step\n",
      "Epoch 77/100\n",
      "\n",
      "Epoch 77: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2742s - loss: 0.1077 - acc: 0.9687 - val_loss: 0.7558 - val_acc: 0.9047 - 2742s/epoch - 1s/step\n",
      "Epoch 78/100\n",
      "\n",
      "Epoch 78: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2736s - loss: 0.1062 - acc: 0.9691 - val_loss: 0.7568 - val_acc: 0.9054 - 2736s/epoch - 1s/step\n",
      "Epoch 79/100\n",
      "\n",
      "Epoch 79: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2742s - loss: 0.1056 - acc: 0.9694 - val_loss: 0.7605 - val_acc: 0.9057 - 2742s/epoch - 1s/step\n",
      "Epoch 80/100\n",
      "\n",
      "Epoch 80: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2752s - loss: 0.1062 - acc: 0.9692 - val_loss: 0.7634 - val_acc: 0.9058 - 2752s/epoch - 1s/step\n",
      "Epoch 81/100\n",
      "\n",
      "Epoch 81: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2749s - loss: 0.1043 - acc: 0.9698 - val_loss: 0.7631 - val_acc: 0.9051 - 2749s/epoch - 1s/step\n",
      "Epoch 82/100\n",
      "\n",
      "Epoch 82: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2715s - loss: 0.1047 - acc: 0.9697 - val_loss: 0.7687 - val_acc: 0.9044 - 2715s/epoch - 1s/step\n",
      "Epoch 83/100\n",
      "\n",
      "Epoch 83: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2659s - loss: 0.1190 - acc: 0.9661 - val_loss: 0.7636 - val_acc: 0.9048 - 2659s/epoch - 1s/step\n",
      "Epoch 84/100\n",
      "\n",
      "Epoch 84: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2628s - loss: 0.1022 - acc: 0.9703 - val_loss: 0.7691 - val_acc: 0.9050 - 2628s/epoch - 1s/step\n",
      "Epoch 85/100\n",
      "\n",
      "Epoch 85: val_loss did not improve from 0.52437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 - 2616s - loss: 0.1020 - acc: 0.9705 - val_loss: 0.7692 - val_acc: 0.9057 - 2616s/epoch - 1s/step\n",
      "Epoch 86/100\n",
      "\n",
      "Epoch 86: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2621s - loss: 0.1043 - acc: 0.9697 - val_loss: 0.7691 - val_acc: 0.9048 - 2621s/epoch - 1s/step\n",
      "Epoch 87/100\n",
      "\n",
      "Epoch 87: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2619s - loss: 0.1028 - acc: 0.9702 - val_loss: 0.7744 - val_acc: 0.9038 - 2619s/epoch - 1s/step\n",
      "Epoch 88/100\n",
      "\n",
      "Epoch 88: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2616s - loss: 0.1028 - acc: 0.9702 - val_loss: 0.7739 - val_acc: 0.9052 - 2616s/epoch - 1s/step\n",
      "Epoch 89/100\n",
      "\n",
      "Epoch 89: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2618s - loss: 0.1031 - acc: 0.9701 - val_loss: 0.7768 - val_acc: 0.9061 - 2618s/epoch - 1s/step\n",
      "Epoch 90/100\n",
      "\n",
      "Epoch 90: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2630s - loss: 0.1023 - acc: 0.9702 - val_loss: 0.7759 - val_acc: 0.9046 - 2630s/epoch - 1s/step\n",
      "Epoch 91/100\n",
      "\n",
      "Epoch 91: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2636s - loss: 0.1018 - acc: 0.9704 - val_loss: 0.7779 - val_acc: 0.9043 - 2636s/epoch - 1s/step\n",
      "Epoch 92/100\n",
      "\n",
      "Epoch 92: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2639s - loss: 0.1014 - acc: 0.9705 - val_loss: 0.7784 - val_acc: 0.9051 - 2639s/epoch - 1s/step\n",
      "Epoch 93/100\n",
      "\n",
      "Epoch 93: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2635s - loss: 0.1005 - acc: 0.9707 - val_loss: 0.7851 - val_acc: 0.9060 - 2635s/epoch - 1s/step\n",
      "Epoch 94/100\n",
      "\n",
      "Epoch 94: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2634s - loss: 0.1016 - acc: 0.9704 - val_loss: 0.7844 - val_acc: 0.9060 - 2634s/epoch - 1s/step\n",
      "Epoch 95/100\n",
      "\n",
      "Epoch 95: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2645s - loss: 0.0999 - acc: 0.9709 - val_loss: 0.7830 - val_acc: 0.9049 - 2645s/epoch - 1s/step\n",
      "Epoch 96/100\n",
      "\n",
      "Epoch 96: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2642s - loss: 0.1336 - acc: 0.9641 - val_loss: 0.7833 - val_acc: 0.9030 - 2642s/epoch - 1s/step\n",
      "Epoch 97/100\n",
      "\n",
      "Epoch 97: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2718s - loss: 0.1450 - acc: 0.9594 - val_loss: 0.7723 - val_acc: 0.9047 - 2718s/epoch - 1s/step\n",
      "Epoch 98/100\n",
      "\n",
      "Epoch 98: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2756s - loss: 0.1030 - acc: 0.9700 - val_loss: 0.7760 - val_acc: 0.9057 - 2756s/epoch - 1s/step\n",
      "Epoch 99/100\n",
      "\n",
      "Epoch 99: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2777s - loss: 0.0942 - acc: 0.9726 - val_loss: 0.7835 - val_acc: 0.9051 - 2777s/epoch - 1s/step\n",
      "Epoch 100/100\n",
      "\n",
      "Epoch 100: val_loss did not improve from 0.52437\n",
      "2000/2000 - 2784s - loss: 0.0959 - acc: 0.9722 - val_loss: 0.7840 - val_acc: 0.9046 - 2784s/epoch - 1s/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def define_model_1(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True)) \n",
    "\tmodel.add(Bidirectional(GRU(n_units)))\n",
    "\tmodel.add(RepeatVector(tar_timesteps))\n",
    "\tmodel.add(Bidirectional(GRU(n_units, return_sequences=True)))\n",
    "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "\treturn model\n",
    "    \n",
    "ds_filename, train_ds_fn, test_ds_fn = 'dataset/english-spanish-both-VF-80000.txt', 'dataset/english-spanish-train-VF-80000.txt','dataset/english-spanish-test-VF-80000.txt'\n",
    "units = 128\n",
    "learning_rate = 0.001\n",
    "loss_func='sparse_categorical_crossentropy'\n",
    "epochs = 100\n",
    "batch_size=32\n",
    "model_save_file_name='best_model-VF-80000.h5'\n",
    "\n",
    "dataset,train,test=load_data(ds_filename, train_ds_fn, test_ds_fn)\n",
    "\n",
    "eng_tokenizer,eng_vocab_size,eng_max_sentence_length = prepare_tokenizer(dataset,0)\n",
    "spa_tokenizer,spa_vocab_size,spa_max_sentence_length = prepare_tokenizer(dataset,1)\n",
    "print(eng_vocab_size,spa_vocab_size,eng_max_sentence_length,spa_max_sentence_length)\n",
    "\n",
    "trainX, trainY =  preprocess_input(eng_tokenizer, eng_max_sentence_length, spa_tokenizer, spa_max_sentence_length,spa_vocab_size, train, one_hot=False)\n",
    "testX, testY = preprocess_input(eng_tokenizer, eng_max_sentence_length, spa_tokenizer, spa_max_sentence_length,spa_vocab_size, test, one_hot=False)\n",
    "print(trainX.shape, trainY.shape, testX.shape, testY.shape)\n",
    "print(trainX.shape, trainY.shape, testX.shape, testY.shape)\n",
    "\n",
    "model_history_save_file_name ='best_model-VF-80000.npy'\n",
    "best_model = define_model_1(eng_vocab_size, spa_vocab_size, eng_max_sentence_length, spa_max_sentence_length, units)\n",
    "create_model(best_model,loss_func,learning_rate)\n",
    "plot_model(best_model, to_file='best_model-VF-80000.png', show_shapes=True)\n",
    "train_evaluate_model(trainX, trainY, testX,testY, epochs, batch_size, best_model, model_save_file_name,model_history_save_file_name)\n",
    "graph_loss_vs_epochs(best_model.history, 'best_model-VF-80000.png', 'best_model-VF-80000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5984ff0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empezo 9:24 - 20:48 --> 16 epochs\n",
    "--> 9:51 --> epoch 34\n",
    "--> 10:17 -->epoch 35\n",
    "--> 12:55 -->epoch 38\n",
    "--> 23:16 --> epoch 51\n",
    "    next day\n",
    "--> 11:45 epoch 68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1e2647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ebac3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911f64d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71cdc4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f21604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a768cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8202cdfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caf425c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89cf917",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9c325b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45591a54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfb4f19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75545f6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ae8ec6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73b5f01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362c2f07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "71f2d036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12066 22997 42 44\n",
      "(80000, 42) (80000, 44) (20000, 42) (20000, 44)\n",
      "(80000, 42) (80000, 44) (20000, 42) (20000, 44)\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_7 (Embedding)     (None, 42, 128)           1544448   \n",
      "                                                                 \n",
      " bidirectional_14 (Bidirecti  (None, 256)              198144    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " repeat_vector_7 (RepeatVect  (None, 44, 256)          0         \n",
      " or)                                                             \n",
      "                                                                 \n",
      " bidirectional_15 (Bidirecti  (None, 44, 256)          296448    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " time_distributed_7 (TimeDis  (None, 44, 22997)        5910229   \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,949,269\n",
      "Trainable params: 7,949,269\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n",
      "Epoch 1/100\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.96640, saving model to Models\\best_model-VF.h5\n",
      "2500/2500 - 4070s - loss: 1.0550 - acc: 0.8665 - val_loss: 0.9664 - val_acc: 0.8700 - 4070s/epoch - 2s/step\n",
      "Epoch 2/100\n",
      "\n",
      "Epoch 2: val_loss improved from 0.96640 to 0.88229, saving model to Models\\best_model-VF.h5\n",
      "2500/2500 - 4061s - loss: 0.9165 - acc: 0.8732 - val_loss: 0.8823 - val_acc: 0.8764 - 4061s/epoch - 2s/step\n",
      "Epoch 3/100\n",
      "\n",
      "Epoch 3: val_loss improved from 0.88229 to 0.76419, saving model to Models\\best_model-VF.h5\n",
      "2500/2500 - 4058s - loss: 0.8099 - acc: 0.8800 - val_loss: 0.7642 - val_acc: 0.8838 - 4058s/epoch - 2s/step\n",
      "Epoch 4/100\n",
      "\n",
      "Epoch 4: val_loss improved from 0.76419 to 0.66545, saving model to Models\\best_model-VF.h5\n",
      "2500/2500 - 4059s - loss: 0.6756 - acc: 0.8883 - val_loss: 0.6655 - val_acc: 0.8907 - 4059s/epoch - 2s/step\n",
      "Epoch 5/100\n",
      "\n",
      "Epoch 5: val_loss improved from 0.66545 to 0.61144, saving model to Models\\best_model-VF.h5\n",
      "2500/2500 - 4064s - loss: 0.5698 - acc: 0.8949 - val_loss: 0.6114 - val_acc: 0.8940 - 4064s/epoch - 2s/step\n",
      "Epoch 6/100\n",
      "\n",
      "Epoch 6: val_loss improved from 0.61144 to 0.58289, saving model to Models\\best_model-VF.h5\n",
      "2500/2500 - 4058s - loss: 0.4961 - acc: 0.9001 - val_loss: 0.5829 - val_acc: 0.8972 - 4058s/epoch - 2s/step\n",
      "Epoch 7/100\n",
      "\n",
      "Epoch 7: val_loss improved from 0.58289 to 0.56858, saving model to Models\\best_model-VF.h5\n",
      "2500/2500 - 4035s - loss: 0.4427 - acc: 0.9044 - val_loss: 0.5686 - val_acc: 0.8984 - 4035s/epoch - 2s/step\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5572/2369334472.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[0mplot_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'model_images/best_model-VF.png'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[0mtrain_evaluate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtestY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_save_file_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel_history_save_file_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[0mgraph_loss_vs_epochs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'loss_vs_epochs_images/best_model-VF.png'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'best_model-VF'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Sistemas Inteligentes\\Proyecto AI\\Example 4 - grupal version 1\\fit_model.py\u001b[0m in \u001b[0;36mtrain_evaluate_model\u001b[1;34m(trainX, trainY, testX, testY, epochs, batch_size, model, model_save_file_name, history_save_file_name)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrain_evaluate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtestY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_save_file_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory_save_file_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[0mcheckpoint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_save_file_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msave_best_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'min'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m     \u001b[0msave_history\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory_save_file_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1382\u001b[0m                 _r=1):\n\u001b[0;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1384\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1385\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from fit_model import *\n",
    "def define_model_1(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True)) \n",
    "\tmodel.add(Bidirectional(GRU(n_units)))\n",
    "\tmodel.add(RepeatVector(tar_timesteps))\n",
    "\tmodel.add(Bidirectional(GRU(n_units, return_sequences=True)))\n",
    "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "\treturn model\n",
    "    \n",
    "ds_filename, train_ds_fn, test_ds_fn = 'dataset/english-spanish-both-VF.txt', 'dataset/english-spanish-train-VF.txt','dataset/english-spanish-test-VF.txt'\n",
    "units = 128\n",
    "learning_rate = 0.001\n",
    "loss_func='sparse_categorical_crossentropy'\n",
    "epochs = 100\n",
    "batch_size=32\n",
    "model_save_file_name='Models/best_model-VF.h5'\n",
    "\n",
    "dataset,train,test=load_data(ds_filename, train_ds_fn, test_ds_fn)\n",
    "\n",
    "eng_tokenizer,eng_vocab_size,eng_max_sentence_length = prepare_tokenizer(dataset,0)\n",
    "spa_tokenizer,spa_vocab_size,spa_max_sentence_length = prepare_tokenizer(dataset,1)\n",
    "print(eng_vocab_size,spa_vocab_size,eng_max_sentence_length,spa_max_sentence_length)\n",
    "\n",
    "trainX, trainY =  preprocess_input(eng_tokenizer, eng_max_sentence_length, spa_tokenizer, spa_max_sentence_length,spa_vocab_size, train, one_hot=False)\n",
    "testX, testY = preprocess_input(eng_tokenizer, eng_max_sentence_length, spa_tokenizer, spa_max_sentence_length,spa_vocab_size, test, one_hot=False)\n",
    "print(trainX.shape, trainY.shape, testX.shape, testY.shape)\n",
    "print(trainX.shape, trainY.shape, testX.shape, testY.shape)\n",
    "\n",
    "model_history_save_file_name ='model_history/best_model-VF.npy'\n",
    "best_model = define_model_1(eng_vocab_size, spa_vocab_size, eng_max_sentence_length, spa_max_sentence_length, units)\n",
    "create_model(best_model,loss_func,learning_rate)\n",
    "plot_model(best_model, to_file='model_images/best_model-VF.png', show_shapes=True)\n",
    "train_evaluate_model(trainX, trainY, testX,testY, epochs, batch_size, best_model, model_save_file_name,model_history_save_file_name)\n",
    "graph_loss_vs_epochs(best_model.history, 'loss_vs_epochs_images/best_model-VF.png', 'best_model-VF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91255602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empezo 12:25 , 8:25 ---> 7 epoch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dcd7d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1517bb75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10b08b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb0b868",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5827ee57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model_1(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True)) \n",
    "\tmodel.add(Bidirectional(GRU(n_units)))\n",
    "\tmodel.add(RepeatVector(tar_timesteps))\n",
    "\tmodel.add(Bidirectional(GRU(n_units, return_sequences=True)))\n",
    "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "\treturn model\n",
    "    \n",
    "ds_filename, train_ds_fn, test_ds_fn = 'dataset/english-spanish-both-100000.txt', 'dataset/english-spanish-train-100000.txt','dataset/english-spanish-test-100000.txt'\n",
    "units = 128\n",
    "learning_rate = 0.001\n",
    "loss_func='sparse_categorical_crossentropy'\n",
    "epochs = 100\n",
    "batch_size=32\n",
    "model_save_file_name='Models/best_model-100000.h5'\n",
    "\n",
    "dataset,train,test=load_data(ds_filename, train_ds_fn, test_ds_fn)\n",
    "\n",
    "eng_tokenizer,eng_vocab_size,eng_max_sentence_length = prepare_tokenizer(dataset,0)\n",
    "spa_tokenizer,spa_vocab_size,spa_max_sentence_length = prepare_tokenizer(dataset,1)\n",
    "print(eng_vocab_size,spa_vocab_size,eng_max_sentence_length,spa_max_sentence_length)\n",
    "\n",
    "trainX, trainY =  preprocess_input(eng_tokenizer, eng_max_sentence_length, spa_tokenizer, spa_max_sentence_length,spa_vocab_size, train, one_hot=False)\n",
    "testX, testY = preprocess_input(eng_tokenizer, eng_max_sentence_length, spa_tokenizer, spa_max_sentence_length,spa_vocab_size, test, one_hot=False)\n",
    "print(trainX.shape, trainY.shape, testX.shape, testY.shape)\n",
    "print(trainX.shape, trainY.shape, testX.shape, testY.shape)\n",
    "\n",
    "model_history_save_file_name ='model_history/best_model-100000.npy'\n",
    "best_model = define_model_1(eng_vocab_size, spa_vocab_size, eng_max_sentence_length, spa_max_sentence_length, units)\n",
    "create_model(best_model,loss_func,learning_rate)\n",
    "plot_model(best_model, to_file='model_images/best_model-100000.png', show_shapes=True)\n",
    "train_evaluate_model(trainX, trainY, testX,testY, epochs, batch_size, best_model, model_save_file_name,model_history_save_file_name)\n",
    "graph_loss_vs_epochs(best_model.history, 'loss_vs_epochs_images/best_model-100000.png', 'best_model-100000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64d39dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empeso 9:36 pm , primer epoch 11:30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8f6e8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
