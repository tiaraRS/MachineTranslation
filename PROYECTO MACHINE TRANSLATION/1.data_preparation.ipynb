{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPARING DATA:\n",
    "### Load dataset\n",
    "Note: The dataset file is not in the repository since it is too heavy. You can download the data by running the following 2 cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fit_model import *\n",
    "import string\n",
    "from pickle import dump\n",
    "from unicodedata import normalize\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_file = keras.utils.get_file(\n",
    "#    fname=\"spa-eng.zip\",\n",
    "#    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n",
    "#    extract=True,\n",
    "#)\n",
    "#text_file = pathlib.Path(text_file).parent / \"spa-eng\" / \"spa.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Besides loading the data which had samples as following\n",
    "english sentence             spanish sentence\n",
    "#### we generated a second dataset inversely for translation spanish to english:\n",
    "spanish sentence             english sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Dataset\n",
    "Cleaning the data involves:\n",
    "1. Getting rid of lines that contain digits\n",
    "2. Converting words to lowercase\n",
    "3. Normalizing special characters to ASCII(such as spanish accents)\n",
    "4. Erasing the punctuation from each line\n",
    "5. Getting rid of lines that contain non-alphabetical characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['go' 've']\n",
      " ['go' 'vete']\n",
      " ['go' 'vaya']\n",
      " ['go' 'vayase']\n",
      " ['hi' 'hola']\n",
      " ['run' 'corre']\n",
      " ['run' 'corred']\n",
      " ['who' 'quien']\n",
      " ['fire' 'fuego']\n",
      " ['fire' 'incendio']]\n",
      "Saved: spanish-english-clean.txt\n"
     ]
    }
   ],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, mode='rt', encoding='utf-8')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# split a loaded document into sentences\n",
    "def to_pairs(doc):\n",
    "\tlines = doc.strip().split('\\n') #get each line\n",
    "\tpairs = [line.split('\\t') for line in  lines] # split into pairs - english-spanish\n",
    "\treturn pairs\n",
    "\n",
    "# clean a list of lines\n",
    "def clean_pairs(lines):\n",
    "\tcleaned = list()\n",
    "\tfor pair in lines:\n",
    "\t\tpair_string = \"\".join(pair)\n",
    "\n",
    "\t\t# delete line if it has a digit\n",
    "\t\tif len([c for c in pair_string if c.isdigit()])>0:\n",
    "\t\t\tcontinue\n",
    "\t\tclean_pair = list()\n",
    "\t\tfor line in pair:\t\t\n",
    "\t\t\t# normalize unicode characters\n",
    "\t\t\tline = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "\t\t\tline = line.decode('UTF-8')\n",
    "\t\t\t# tokenize on white space\n",
    "\t\t\tline = line.split() #converts to array with each word\n",
    "\t\t\t# convert to lowercase\n",
    "\t\t\tline = [word.lower() for word in line]\n",
    "\t\t\t# remove punctuation from each token\n",
    "\t\t\tline = [word.strip(string.punctuation) for word in line]\n",
    "\t\t\tline = [word for word in line if word.isalpha()]\n",
    "\t\t\t# store as string\n",
    "\t\t\tclean_pair.append(' '.join(line))\n",
    "\t\tcleaned.append(clean_pair)\n",
    "\treturn array(cleaned)\n",
    "\n",
    "# save a list of clean sentences to file\n",
    "def save_clean_data(sentences, filename):\n",
    "\tdump(sentences, open(filename, 'wb'))\n",
    "\tprint('Saved: %s' % filename)\n",
    "\n",
    "# load dataset\n",
    "filename = 'spa-eng/spa.txt'\n",
    "doc = load_doc(filename)\n",
    "# split into english-german pairs\n",
    "pairs = to_pairs(doc)\n",
    "# clean sentences\n",
    "clean_pairs = clean_pairs(pairs)\n",
    "print(clean_pairs[:10])\n",
    "# save clean pairs to file\n",
    "save_clean_data(clean_pairs, 'spanish-english-clean.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide the clean Dataset into Training and Testing set:\n",
    "    We divided the dataset using\n",
    "    - 80% of original dataset for training\n",
    "    - 20% of original dataset for testing\n",
    "#### For our experiments, we selected the first 10 000 lines from the dataset, and shuffled them, then selected the first 8000 lines for training and the last 2000 for testing.\n",
    "#### For our final models, we first shuffled the whole dataset, then used 80 000 lines, selecting the first 64 000 lines for training and the last 16 000 for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117788\n",
      "800 200\n",
      "Saved: datasets/english-spanish-both-1000.txt\n",
      "Saved: datasets/english-spanish-train-1000.txt\n",
      "Saved: datasets/english-spanish-test-1000.txt\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "from pickle import dump\n",
    "from numpy.random import rand\n",
    "from numpy.random import shuffle\n",
    "\n",
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "\treturn load(open(filename, 'rb'))\n",
    "\n",
    "# save a list of clean sentences to file\n",
    "def save_clean_data(sentences, filename):\n",
    "\tdump(sentences, open(filename, 'wb'))\n",
    "\tprint('Saved: %s' % filename)\n",
    "\n",
    "# load dataset\n",
    "raw_dataset = load_clean_sentences('spanish-english-clean.txt')\n",
    "dataset_number_of_samples = raw_dataset.shape[0]\n",
    "print(dataset_number_of_samples)\n",
    "# reduce dataset size\n",
    "n_sentences = 1000\n",
    "dataset = raw_dataset[:n_sentences, :]\n",
    "# random shuffle\n",
    "shuffle(dataset)\n",
    "# split into train/test\n",
    "index_80=int(n_sentences*0.8)\n",
    "train, test = dataset[:index_80], dataset[index_80:]\n",
    "print(len(train),len(test))\n",
    "# save\n",
    "save_clean_data(dataset, 'datasets/english-spanish-both-1000.txt')\n",
    "save_clean_data(train, 'datasets/english-spanish-train-1000.txt')\n",
    "save_clean_data(test, 'datasets/english-spanish-test-1000.txt')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aba33f5906df6e7b2943b50860cc645261c57a13f9a1e1d4acadf1f03aacb37d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
