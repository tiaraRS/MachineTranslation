{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "307e6795",
   "metadata": {},
   "source": [
    "    We followed the tutorial from https://machinelearningmastery.com/develop-neural-machine-translation-system-keras/\n",
    "    in order to build our machine translator, adapting it to use our english-spanish dataset.\n",
    "    \n",
    "    ##### Libraries needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e67de694",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "from numpy import array\n",
    "import tensorflow\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "#from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2471534a",
   "metadata": {},
   "source": [
    "#### Data Preparation for the Neural Network:\n",
    "    The goal is to have a sequence of words in english get transformed to a sequence of words in spanish. In this case, the input (x) of the neural network are the sentences in english, and the ouput should be the sentences in english(y). \n",
    "    Since neural networks do not understand text, we need to transorm them into numbers:\n",
    "    In order to do this, we will create a tokenizer for each language, from the vocabulary in the dataset.\n",
    "    A tokenizer creates a numerical id for each word in its vocabulary, therefore each word will be represented by a number. This will be donde for both languages, (x and y). \n",
    "    So, redefining what we said before, our input will be the corresponding ids (referenced by the tokenizer) from the sentences in english, and our outputs will be the probability of each each input word corresponding to the vocabulary of the target language(spanish). We will perform the steps to do this:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0edd0059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_sentences(filename):\n",
    "\treturn load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a78bc9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(lines): #assigns id to words in lines vocab\n",
    "\ttokenizer = Tokenizer() #default filters punctuation\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00b9206",
   "metadata": {},
   "source": [
    "##### Simple example to show how a tokenizer works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c986f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = ['hola que tal tal tal.', 'como estas hoy hola', 'mi perrito bello es','vamos a comer silpancho mas tarde']\n",
    "t = create_tokenizer(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a3c8df",
   "metadata": {},
   "source": [
    "##### Viewing the ids assigned to each word from the example vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff0c72c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document count 4\n",
      "The count of words 16\n",
      "The word index {'tal': 1, 'hola': 2, 'que': 3, 'como': 4, 'estas': 5, 'hoy': 6, 'mi': 7, 'perrito': 8, 'bello': 9, 'es': 10, 'vamos': 11, 'a': 12, 'comer': 13, 'silpancho': 14, 'mas': 15, 'tarde': 16}\n"
     ]
    }
   ],
   "source": [
    "print(\"The document count\",t.document_count)\n",
    "print(\"The count of words\",len(t.word_counts))\n",
    "print(\"The word index\",t.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73077b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(lines): #max words in a sentence\n",
    "\treturn max(len(line.split()) for line in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21e734f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69070c6",
   "metadata": {},
   "source": [
    "##### SENTENCE ENCODING:\n",
    "    Once the tokenizer is created, we have to map the dataset's sentences into the ids from the tokenizer.\n",
    "    Additionaly, for the input arrays to be the same length(the max length from the dataset sentences), we add 0s at the end as padding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b57e1602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sequences(tokenizer, length, lines): #returns dim (#lines,length)\n",
    "\t# integer encode sequences\n",
    "\tX = tokenizer.texts_to_sequences(lines)\n",
    "\t#creates id array from tokenizer ids for each sentence\n",
    "\t# pad sequences with 0 values\n",
    "\tX = pad_sequences(X, maxlen=length, padding='post') # adds 0s to the end(post) of sequence\n",
    "\treturn X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0b30a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 10)\n",
      "[[ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 2  4  5  0  0  0  0  0  0  0]\n",
      " [11  0  0  0  0  0  0  0  0  0]\n",
      " [ 3  1  5  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "ba=encode_sequences(t,10, [\"asd sdjid\",\"hola como estas\", \"chau vamos\", \"que tal estas\"])\n",
    "print(ba.shape)\n",
    "print(ba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5dd365",
   "metadata": {},
   "source": [
    "##### The tutorial we followed used the output y as one hot encoding. This could be used for small number of data, but we will try to scale the dataset later, so this is not convenient for us. Anyway, we did use it for the best experiment we found, to compare it with the simple array output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29c74d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]], shape=(4, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#to_categorical : builds one-hot encoding representation for input array\n",
    "a = tensorflow.keras.utils.to_categorical([0, 1, 2, 3], num_classes=4)\n",
    "a = tensorflow.constant(a, shape=[4, 4])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ca7e790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_output(sequences, vocab_size): #returns 3d\n",
    "\tylist = np.array([])\n",
    "\tfor sequence in sequences:\n",
    "\t\tencoded = to_categorical(sequence, num_classes=vocab_size,dtype=int)\n",
    "\t\tylist=np.append(ylist,encoded)\n",
    "\ty = array(ylist)\n",
    "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "\treturn y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728acaaa",
   "metadata": {},
   "source": [
    "##### As we can see in the following exmple, even for a small vocabulary, we have a big one hot matrix already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7805f6e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_output(encode_sequences(t,10,[\"fine thanks\",\"hola como estas\",\"hola que como tal como\"]), 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702f0da8",
   "metadata": {},
   "source": [
    "##### Sample model:\n",
    "    This was the base model from the tutorial:\n",
    "    LAYERS:\n",
    "        -EMBEDDING\n",
    "        -LSTM\n",
    "        -REPEAT VECTOR\n",
    "        -LSTM\n",
    "        -TIME DISTRIBUTED (DENSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac7ac2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#none = batch dimension\n",
    "# batch size = # of samples in each batch during testing/training\n",
    "# timestemps = # of values in a sequence -> max # of words in sentences\n",
    "# features = # of dimensions to represent data\n",
    "#\t\t\t\t\t3856      2404       10            5              256\t\t\t\t\t\t\t\n",
    "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "\tmodel = Sequential()\n",
    "\t#src_vocab shape = (#german_sent, max#words in germ sentence)\n",
    "\t#src_vocab shape = (3856, 10)\n",
    "\t#src_vocab = SIZE  = 3856\n",
    "\n",
    "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True)) \n",
    "\t# 3856, 256 units outp, 10, \n",
    "\t# mask_zero -> tells model 0 is a padding number and cannot be used as index for vocabulary\n",
    "\t# input dim = vocab size + 1\n",
    "\n",
    "\t#EMBEDDING LAYER : \n",
    "\t# \tinput(batch_size, input_length) \n",
    "\t# \t->output (batch_size, input_length, output_dim)\n",
    "\t# (,10,256) - params = 3856x256\n",
    "\n",
    "\t#model.add(SimpleRNN(n_units,activation='relu'))\n",
    "\t#model.add(SimpleRNN())\n",
    "\t\n",
    "\n",
    "\tmodel.add(LSTM(n_units))\n",
    "\t#input 3D [batch_size, timesteps, feature] -> (,10,256)\n",
    "\t# output ,256 - params = \n",
    "\n",
    "\tmodel.add(RepeatVector(tar_timesteps))\n",
    "\t# repeats input n times\n",
    "\t# input = (batch_size, input_length, output_dim)\n",
    "\t# tar_timesteps = english max word # in a sentence\n",
    "\t# output = (batch_size, tar_timesteps, output_dim)\n",
    "\n",
    "\t#model.add(SimpleRNN(n_units, return_sequences=True))\n",
    "\tmodel.add(LSTM(n_units, return_sequences=True))#default sigmoid activation\n",
    "\t# return_sequence = boolean , return full sequence if true, else return just output sequence\n",
    "\t# input (batch_size, input_length, c)\n",
    "\t# output \n",
    "\t# \treturn_seq TRUE:(batch_size, input_length, 256) - (,5,256)\n",
    "\t#\treturn_seq FALSE:(batch_size,256)\n",
    "\n",
    "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "\t# input : (batch_size, input_length, 256)\n",
    "\t# output: (batch, batch_size, input_length, 256)\n",
    "\t# outputs should have same function for every timestep, we dont want flattened output\n",
    "\t# softmax returns probability array\n",
    "\t# Dense output: tar_vocab legnth array of probabilities for each word\n",
    "\t# 256x2404 + 2404(biases) = 618828\n",
    "\n",
    "\t#model.add(Dense(tar_vocab, activation='softmax'))\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d600d75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape inputs (32, 10, 8)\n"
     ]
    }
   ],
   "source": [
    "inputs = tensorflow.random.normal([32, 10, 8])\n",
    "print(\"shape inputs\", inputs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e833e9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(ds_filename, train_ds_fn, test_ds_fn):    \n",
    "    dataset = load_clean_sentences(ds_filename)\n",
    "    train = load_clean_sentences(train_ds_fn)\n",
    "    test = load_clean_sentences(test_ds_fn)\n",
    "    return dataset,train,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ba2be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_tokenizer(dataset):\n",
    "    tokenizer = create_tokenizer(dataset[:, 0])\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    max_sentence_length = max_length(dataset[:, 0])\n",
    "    return tokenizer,vocab_size,max_sentence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abcb831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input(origin_tok, origin_max_sent_length, target_tok, target_max_sent_length,target_vocab_size, data, one_hot=False):\n",
    "    dataX = encode_sequences(origin_tok, origin_max_sent_length, data[:, 1])\n",
    "    dataY = encode_sequences(target_tok, target_max_sent_length, data[:, 0])\n",
    "    if one_hot:\n",
    "        dataY = encode_output(dataY, target_vocab_size)\n",
    "    return dataX,dataY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fcc9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def graph_loss_vs_epochs(history):\n",
    "    training_loss = history.history['loss']\n",
    "    test_loss = history.history['val_loss']\n",
    "\n",
    "    # Create count of the number of epochs\n",
    "    epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "    # Visualize loss history\n",
    "    plt.plot(epoch_count, training_loss, 'r--')\n",
    "    plt.plot(epoch_count, test_loss, 'b-')\n",
    "    plt.legend(['Training Loss', 'Test Loss'])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9635251c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_model(trainX, trainY, testX,testY, epochs, batch_size, model, model_save_file_name):\n",
    "    checkpoint = ModelCheckpoint(model_save_file_name, monitor='val_loss', verbose=1,save_best_only=True, mode='min')\n",
    "    model.fit(trainX, trainY, epochs=epochs, batch_size=batch_size,  validation_data=(testX, testY))\n",
    "    history=model.history\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be7b236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model, units,model_sum_im_name,loss_func='categorical_crossentropy',learning_rate=0.001):\n",
    "    optimizer = keras.optimizers.Adam(learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=loss_func,metrics=['acc'])\n",
    "    #categorical cross entropy -> one hot encoding output\n",
    "    #sparse categorical cross entropy -> output as integers\n",
    "    # summarize defined model\n",
    "    print(model.summary())\n",
    "    plot_model(model, to_file=model_sum_im_name, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c373e7ee",
   "metadata": {},
   "source": [
    "##### Steps for training the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7cf3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset,train,testset=load_data(ds_filename, train_ds_fn, test_ds_fn)\n",
    "tokenizer,vocab_size,max_sentence_length = prepare_tokenizer(dataset)\n",
    "model = define_model(origin_vocab_size, target_vocab_size, origin_max_sent_length, target_max_sent_length, units)\n",
    "create_model(model, units,loss_func='categorical_crossentropy',learning_rate=0.001)\n",
    "train_evaluate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "612c570d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "dataset = load_clean_sentences('english-spanish-both.txt')\n",
    "train = load_clean_sentences('english-spanish-train.txt')\n",
    "test = load_clean_sentences('english-spanish-test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97e8872c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 2) (2000, 2) (117788, 2)\n",
      "[['fine today' 'hoy hace bueno']\n",
      " ['can we go' 'podemos ir']\n",
      " ['you were busy' 'estabas ocupada']\n",
      " ['can you whistle' 'podes silbar']\n",
      " ['tom speak' 'tom no puede hablar']\n",
      " ['it like' 'como es']\n",
      " ['french' 'soy frances']\n",
      " ['i like them all' 'me gustan todos']\n",
      " ['it was my fault' 'fue mi culpa']\n",
      " ['who sells this' 'quien vende esto']\n",
      " ['is it a trap' 'acaso es una trampa']\n",
      " ['on a diet' 'estoy a dieta']\n",
      " ['new' 'estan nuevas']\n",
      " ['tom might faint' 'tom quizas podria desfallecer']\n",
      " ['tom saw me' 'tom me vio']\n",
      " ['how was boston' 'que tal en boston']\n",
      " ['how do you feel' 'como os sentis']\n",
      " ['do you trust me' 'confias en mi']\n",
      " ['go get it' 'vete a por ello']\n",
      " ['do it again' 'lo volveria a hacer']\n",
      " ['what a jerk' 'que pendejo']\n",
      " ['tom did not cry' 'tom no lloro']\n",
      " ['a book' 'eso es un libro']\n",
      " ['this is a pen' 'esto es un boligrafo']\n",
      " ['know' 'lo sabran']\n",
      " ['dad home' 'papa no esta en casa']\n",
      " ['i need ice' 'necesito hielo']\n",
      " ['i live in hyogo' 'yo vivo en hyogo']\n",
      " ['mine' 'es mia']\n",
      " ['rather walk' 'preferiria caminar']\n",
      " ['she kissed him' 'ella le beso']\n",
      " ['she trusted you' 'ella confiaba en ti']\n",
      " ['undressing' 'me estoy desnudando']\n",
      " ['excited' 'estoy emocionado']\n",
      " ['listen' 'escuche']\n",
      " ['can i help you' 'te puedo ayudar']\n",
      " ['the birds sang' 'los pajaros cantaban']\n",
      " ['with me' 'ellas estan conmigo']\n",
      " ['continue working' 'seguid trabajando']\n",
      " ['you may go' 'podeis iros']\n",
      " ['give me half' 'dame la mitad']\n",
      " ['go up' 'vamos para arriba']\n",
      " ['i need a weapon' 'necesito un arma']\n",
      " ['now sad' 'ahora estoy triste']\n",
      " ['i want a lawyer' 'quiero un abogado']\n",
      " ['useless' 'es inutil']\n",
      " ['do you play golf' 'juegas golf']\n",
      " ['they found out' 'ellos lo descubrieron']\n",
      " ['he mentioned it' 'el lo menciono']\n",
      " ['cherries are red' 'las cerezas son rojas']\n",
      " ['i am curious' 'tengo curiosidad']\n",
      " ['not happy' 'no soy feliz']\n",
      " ['go away' 'largate']\n",
      " ['i need them' 'los necesito']\n",
      " ['remove your hat' 'quitate el sombrero']\n",
      " ['can you see it' 'puedes verlo']\n",
      " ['stand back' 'retrocede']\n",
      " ['tom escaped' 'tom escapo']\n",
      " ['just follow me' 'tan solo sigueme']\n",
      " ['try me' 'no me tientes']\n",
      " ['i met a friend' 'me encontre con una amiga']\n",
      " ['go by car' 'ire en coche']\n",
      " ['not perfect' 'no soy perfecta']\n",
      " ['i use firefox' 'yo uso firefox']\n",
      " ['did you know tom' 'conocias a tom']\n",
      " ['a good cook' 'se me da bien la cocina']\n",
      " ['turn on the tv' 'enciende la tele']\n",
      " ['no singer' 'ella no es ninguna cantante']\n",
      " ['tom loves dogs' 'tom ama a los perros']\n",
      " ['just do that' 'tan solo hazlo']\n",
      " ['i felt reborn' 'yo me senti renacido']\n",
      " ['seize him' 'a por el']\n",
      " ['cook for me' 'cociname']\n",
      " ['eat here' 'comere aqui']\n",
      " ['is anybody hurt' 'se lastimo alguien']\n",
      " ['i had doubts' 'yo tenia dudas']\n",
      " ['tom waved' 'tom saludo']\n",
      " ['come on' 'orale']\n",
      " ['they loved you' 'te querian']\n",
      " ['tom now' 'donde esta tom ahora']\n",
      " ['life easy' 'la vida no es facil']\n",
      " ['where is he' 'donde esta el']\n",
      " ['i made a deal' 'yo hice un trato']\n",
      " ['what a phony' 'que farsante']\n",
      " ['let me in' 'dejeme entrar']\n",
      " ['i saw him jump' 'le vi saltar']\n",
      " ['reward you' 'te premiare']\n",
      " ['an adult' 'soy adulta']\n",
      " ['he is very fast' 'el es muy rapido']\n",
      " ['please hold on' 'por favor espere']\n",
      " ['just look at me' 'mirame']\n",
      " ['i need time' 'necesito tiempo']\n",
      " ['i am studying' 'estoy estudiando']\n",
      " ['tom shot mary' 'tom le disparo a mary']\n",
      " ['we failed' 'hemos fallado']\n",
      " ['my treat' 'es de parte mia']\n",
      " ['tom is at work' 'tom esta en el trabajo']\n",
      " ['i need a car' 'necesito un coche']\n",
      " ['not a hero' 'no soy un heroe']\n",
      " ['please sing' 'canta por favor']]\n"
     ]
    }
   ],
   "source": [
    "print(train.shape, test.shape, dataset.shape)\n",
    "print(train[100:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85444ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 12715\n",
      "English Max Length: 47\n"
     ]
    }
   ],
   "source": [
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "print('English Max Length: %d' % (eng_length))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c0089be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spanish Vocabulary Size: 24581\n",
      "Spanish Max Length: 49\n"
     ]
    }
   ],
   "source": [
    "# prepare german tokenizer\n",
    "spa_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "spa_vocab_size = len(spa_tokenizer.word_index) + 1\n",
    "spa_length = max_length(dataset[:, 1])\n",
    "print('Spanish Vocabulary Size: %d' % spa_vocab_size)\n",
    "print('Spanish Max Length: %d' % (spa_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd28bb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare validation data\n",
    "testX = encode_sequences(spa_tokenizer, spa_length, test[:, 1])\n",
    "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
    "testY = encode_output(testY, eng_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7145af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#usa las primeras 1000 oraciones\n",
    "# define model\n",
    "learning_rate=0.001\n",
    "model = define_model(spa_vocab_size, eng_vocab_size, spa_length, eng_length, 256)\n",
    "optimizer = keras.optimizers.Adam(learning_rate)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy',metrics=['acc'])\n",
    "#categorical cross entropy -> one hot encoding output\n",
    "#sparse categorical cross entropy -> output as integers\n",
    "# summarize defined model\n",
    "print(model.summary())\n",
    "#plot_model(model, to_file='model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ed7b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "#filename = 'model.h5'\n",
    "#checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "model.fit(trainX, trainY, epochs=30, batch_size=64,  validation_data=(testX, testY))#, callbacks=[checkpoint])#, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe02e64",
   "metadata": {},
   "source": [
    "#### Making a prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53449a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import argmax\n",
    "# asignar un n√∫mero entero a una palabra\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    " \n",
    "# generar la secuencia de origen del objetivo\n",
    "def predict_sequence(model, tokenizer, source):\n",
    "    #print(f\"S {source}\")\n",
    "    prediction = model.predict(source, verbose=0)[0]\n",
    "    #print(f\"PREDICTION {prediction}\")\n",
    "    integers = [argmax(vector) for vector in prediction]\n",
    "    target = list()\n",
    "    for i in integers:\n",
    "        word = word_for_id(i, tokenizer)\n",
    "        #print(f\"WORD{word}\")\n",
    "        if word is None:\n",
    "            break\n",
    "        target.append(word)\n",
    "    return' '.join(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a10f635",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_prediction_test=encode_sequences(spa_tokenizer, spa_length, ['Hola como estas?'])\n",
    "print(x_prediction_test)\n",
    "print(x_prediction_test[0])\n",
    "predict_sequence(model,eng_tokenizer,x_prediction_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
