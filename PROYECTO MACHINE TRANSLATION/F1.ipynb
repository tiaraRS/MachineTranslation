{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e67de694",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "from numpy import array\n",
    "import tensorflow\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "#from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0edd0059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_sentences(filename):\n",
    "\treturn load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a78bc9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(lines): #assigns id to words in lines vocab\n",
    "\ttokenizer = Tokenizer() #default filters punctuation\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7c8e221f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tom was pleased' 'turn it off' 'are you busy' 'are you free'\n",
      " 'were all happy' 'i like horses' 'i go jogging' 'go look for it'\n",
      " 'i like them both' 'this is hers']\n"
     ]
    }
   ],
   "source": [
    "print(dataset[:, 0][0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6c986f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = ['hola que tal tal tal.', 'como estas hoy hola', 'mi perrito bello es','vamos a comer silpancho mas tarde']\n",
    "t = create_tokenizer(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ff0c72c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document count 4\n",
      "The count of words 16\n",
      "The word index {'tal': 1, 'hola': 2, 'que': 3, 'como': 4, 'estas': 5, 'hoy': 6, 'mi': 7, 'perrito': 8, 'bello': 9, 'es': 10, 'vamos': 11, 'a': 12, 'comer': 13, 'silpancho': 14, 'mas': 15, 'tarde': 16}\n"
     ]
    }
   ],
   "source": [
    "print(\"The document count\",t.document_count)\n",
    "print(\"The count of words\",len(t.word_counts))\n",
    "print(\"The word index\",t.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "73077b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(lines): #max words in a sentence\n",
    "\treturn max(len(line.split()) for line in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "21e734f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b57e1602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sequences(tokenizer, length, lines): #returns dim (#lines,length)\n",
    "\t# integer encode sequences\n",
    "\tX = tokenizer.texts_to_sequences(lines) #creates id array from tokenizer ids for each sentence\n",
    "\t# pad sequences with 0 values\n",
    "\tX = pad_sequences(X, maxlen=length, padding='post') # adds 0s to the end(post) of sequence\n",
    "\treturn X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e0b30a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 10)\n",
      "[[ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 2  4  5  0  0  0  0  0  0  0]\n",
      " [11  0  0  0  0  0  0  0  0  0]\n",
      " [ 3  1  5  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "ba=encode_sequences(t,10, [\"asd sdjid\",\"hola como estas\", \"chau vamos\", \"que tal estas\"])\n",
    "print(ba.shape)\n",
    "print(ba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "29c74d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]], shape=(4, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#to_categorical : builds one-hot encoding representation for input array\n",
    "a = tensorflow.keras.utils.to_categorical([0, 1, 2, 3], num_classes=4)\n",
    "a = tensorflow.constant(a, shape=[4, 4])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7ca7e790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_output(sequences, vocab_size): #returns 3d\n",
    "\tylist = list()\n",
    "\tfor sequence in sequences:\n",
    "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "\t\tylist.append(encoded)\n",
    "\ty = array(ylist)\n",
    "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "\treturn y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7805f6e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_output(encode_sequences(t,10,[\"fine thanks\",\"hola como estas\",\"hola que como tal como\"]), 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ac7ac2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#none = batch dimension\n",
    "# batch size = # of samples in each batch during testing/training\n",
    "# timestemps = # of values in a sequence -> max # of words in sentences\n",
    "# features = # of dimensions to represent data\n",
    "#\t\t\t\t\t3856      2404       10            5              256\t\t\t\t\t\t\t\n",
    "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "\tmodel = Sequential()\n",
    "\t#src_vocab shape = (#german_sent, max#words in germ sentence)\n",
    "\t#src_vocab shape = (3856, 10)\n",
    "\t#src_vocab = SIZE  = 3856\n",
    "\n",
    "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True)) \n",
    "\t# 3856, 256 units outp, 10, \n",
    "\t# mask_zero -> tells model 0 is a padding number and cannot be used as index for vocabulary\n",
    "\t# input dim = vocab size + 1\n",
    "\n",
    "\t#EMBEDDING LAYER : \n",
    "\t# \tinput(batch_size, input_length) \n",
    "\t# \t->output (batch_size, input_length, output_dim)\n",
    "\t# (,10,256) - params = 3856x256\n",
    "\n",
    "\t#model.add(SimpleRNN(n_units,activation='relu'))\n",
    "\t#model.add(SimpleRNN())\n",
    "\t\n",
    "\n",
    "\tmodel.add(LSTM(n_units))\n",
    "\t#input 3D [batch_size, timesteps, feature] -> (,10,256)\n",
    "\t# output ,256 - params = \n",
    "\n",
    "\tmodel.add(RepeatVector(tar_timesteps))\n",
    "\t# repeats input n times\n",
    "\t# input = (batch_size, input_length, output_dim)\n",
    "\t# tar_timesteps = english max word # in a sentence\n",
    "\t# output = (batch_size, tar_timesteps, output_dim)\n",
    "\n",
    "\t#model.add(SimpleRNN(n_units, return_sequences=True))\n",
    "\tmodel.add(LSTM(n_units, return_sequences=True))#default sigmoid activation\n",
    "\t# return_sequence = boolean , return full sequence if true, else return just output sequence\n",
    "\t# input (batch_size, input_length, c)\n",
    "\t# output \n",
    "\t# \treturn_seq TRUE:(batch_size, input_length, 256) - (,5,256)\n",
    "\t#\treturn_seq FALSE:(batch_size,256)\n",
    "\n",
    "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "\t# input : (batch_size, input_length, 256)\n",
    "\t# output: (batch, batch_size, input_length, 256)\n",
    "\t# outputs should have same function for every timestep, we dont want flattened output\n",
    "\t# softmax returns probability array\n",
    "\t# Dense output: tar_vocab legnth array of probabilities for each word\n",
    "\t# 256x2404 + 2404(biases) = 618828\n",
    "\n",
    "\t#model.add(Dense(tar_vocab, activation='softmax'))\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f0679787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 64, 15) (50, 64, 8)\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn (SimpleRNN)       (None, 64, 32)            1536      \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 64, 8)             264       \n",
      "=================================================================\n",
      "Total params: 1,800\n",
      "Trainable params: 1,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "2/2 [==============================] - 1s 9ms/step - loss: 27.3202\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 24.7909\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 22.8264\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 21.4247\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 20.5684\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 20.0040\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 19.5366\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 18.9351\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 18.2759\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 17.5485\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 16.8323\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 16.1767\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 15.6187\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 15.0102\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 14.4202\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 13.8554\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 13.1831\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 12.4634\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 11.6522\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 10.7428\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 10.1284\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 9.7276\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 9.4924\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 9.3209\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 9.1931\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 9.0938\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 9.0137\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 8.9280\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 8.8510\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 8.7990\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 17ms/step - loss: 8.7622\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 8.7294\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 8.7014\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 8.6840\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 8.6691\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 8.6552\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 8.6448\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 8.6384\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 8.6300\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 8.6206\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 8.6106\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 8.6019\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 8.5957\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 8.5920\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 8.5888\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 8.5859\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 8.5810\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 8.5778\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 8.5752\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 8.5728\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 8.5701\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 8.5677\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 8.5643\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 8.5622\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 8.5603\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 8.5576\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 8.5557\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 8.5541\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 8.5526\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 8.5511\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 8.5498\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 8.5463\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 8.5424\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 8.5380\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 8.5362\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 8.5323\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 8.5306\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 8.5293\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 8.5267\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 8.5214\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 8.5196\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 8.5181\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 8.5159\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 8.5134\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 8.5091\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 8.5069\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 8.5057\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 8.5019\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 8.5004\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 8.4993\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 8.4982\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 8.4972\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 8.4948\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 8.4902\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 8.4890\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 8.4869\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 8.4847\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 8.4838\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 8.4828\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 8.4821\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 8.4766\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 8.4747\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 8.4727\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 8.4718\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 8.4709\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 8.4701\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 8.4693\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 8.4678\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 8.4659\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 8.4644\n",
      "5/5 - 0s\n"
     ]
    }
   ],
   "source": [
    "#PRUEBITA\n",
    "# from numpy import array\n",
    "# import numpy as np\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, TimeDistributed, LSTM\n",
    "# Input_Dim, Output_Dim = 15, 8\n",
    "# Length = 64\n",
    "# Sample_Size = 50\n",
    "# X = np.random.random([Sample_Size,Length,Input_Dim]) #(50,64,15)\n",
    "# y = np.random.random([Sample_Size,Length,Output_Dim]) #(50,64,8)\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(32, input_shape=(64, 15), return_sequences=True)) #(10,64,32)\n",
    "# model.add(TimeDistributed(Dense(8))) #(10,64,8)\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# print(X.shape, y.shape)\n",
    "# print(model.summary())\n",
    "# model.fit(X, y, epochs=100)\n",
    "# result = model.predict(X, batch_size=10, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d600d75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape inputs (32, 10, 8)\n"
     ]
    }
   ],
   "source": [
    "inputs = tensorflow.random.normal([32, 10, 8])\n",
    "print(\"shape inputs\", inputs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "612c570d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "dataset = load_clean_sentences('english-german-both.pkl')\n",
    "train = load_clean_sentences('english-german-train.pkl')\n",
    "test = load_clean_sentences('english-german-test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "85444ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 2404\n",
      "English Max Length: 5\n"
     ]
    }
   ],
   "source": [
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "print('English Max Length: %d' % (eng_length))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7c0089be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "German Vocabulary Size: 3856\n",
      "German Max Length: 10\n"
     ]
    }
   ],
   "source": [
    "# prepare german tokenizer\n",
    "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
    "ger_length = max_length(dataset[:, 1])\n",
    "print('German Vocabulary Size: %d' % ger_vocab_size)\n",
    "print('German Max Length: %d' % (ger_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "69ab42ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training data\n",
    "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
    "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
    "trainY = encode_output(trainY, eng_vocab_size)\n",
    "# x son secuencias encoded,\n",
    "# y son one hot encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "cd28bb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare validation data\n",
    "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
    "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
    "testY = encode_output(testY, eng_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0b7145af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 10, 256)           987136    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "repeat_vector_2 (RepeatVecto (None, 5, 256)            0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 5, 256)            525312    \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 5, 2404)           617828    \n",
      "=================================================================\n",
      "Total params: 2,655,588\n",
      "Trainable params: 2,655,588\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "#usa las primeras 1000 oraciones\n",
    "# define model\n",
    "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['acc'])\n",
    "#categorical cross entropy -> one hot encoding output\n",
    "#sparse categorical cross entropy -> output as integers\n",
    "# summarize defined model\n",
    "print(model.summary())\n",
    "plot_model(model, to_file='model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "66ed7b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "141/141 [==============================] - 32s 186ms/step - loss: 4.4575 - acc: 0.4054 - val_loss: 3.7011 - val_acc: 0.4218\n",
      "Epoch 2/30\n",
      "141/141 [==============================] - 18s 131ms/step - loss: 3.5524 - acc: 0.4351 - val_loss: 3.5454 - val_acc: 0.4386\n",
      "Epoch 3/30\n",
      "141/141 [==============================] - 15s 109ms/step - loss: 3.3974 - acc: 0.4490 - val_loss: 3.4392 - val_acc: 0.4504\n",
      "Epoch 4/30\n",
      "141/141 [==============================] - 14s 96ms/step - loss: 3.2511 - acc: 0.4630 - val_loss: 3.3391 - val_acc: 0.4626\n",
      "Epoch 5/30\n",
      "141/141 [==============================] - 15s 107ms/step - loss: 3.1161 - acc: 0.4747 - val_loss: 3.2578 - val_acc: 0.4716\n",
      "Epoch 6/30\n",
      "141/141 [==============================] - 20s 144ms/step - loss: 2.9860 - acc: 0.4888 - val_loss: 3.1484 - val_acc: 0.4914\n",
      "Epoch 7/30\n",
      "141/141 [==============================] - 25s 177ms/step - loss: 2.8216 - acc: 0.5118 - val_loss: 3.0347 - val_acc: 0.5028\n",
      "Epoch 8/30\n",
      "141/141 [==============================] - 24s 167ms/step - loss: 2.6545 - acc: 0.5346 - val_loss: 2.9082 - val_acc: 0.5258\n",
      "Epoch 9/30\n",
      "141/141 [==============================] - 24s 168ms/step - loss: 2.4786 - acc: 0.5580 - val_loss: 2.7856 - val_acc: 0.5420\n",
      "Epoch 10/30\n",
      "141/141 [==============================] - 20s 140ms/step - loss: 2.3152 - acc: 0.5769 - val_loss: 2.6864 - val_acc: 0.5564\n",
      "Epoch 11/30\n",
      "141/141 [==============================] - 16s 116ms/step - loss: 2.1677 - acc: 0.5958 - val_loss: 2.6015 - val_acc: 0.5690\n",
      "Epoch 12/30\n",
      "141/141 [==============================] - 25s 176ms/step - loss: 2.0364 - acc: 0.6126 - val_loss: 2.5398 - val_acc: 0.5766\n",
      "Epoch 13/30\n",
      "141/141 [==============================] - 24s 174ms/step - loss: 1.9102 - acc: 0.6249 - val_loss: 2.4652 - val_acc: 0.5940\n",
      "Epoch 14/30\n",
      "141/141 [==============================] - 22s 155ms/step - loss: 1.7908 - acc: 0.6386 - val_loss: 2.4238 - val_acc: 0.6018\n",
      "Epoch 15/30\n",
      "141/141 [==============================] - 21s 147ms/step - loss: 1.6818 - acc: 0.6534 - val_loss: 2.3666 - val_acc: 0.6054\n",
      "Epoch 16/30\n",
      "141/141 [==============================] - 16s 111ms/step - loss: 1.5760 - acc: 0.6663 - val_loss: 2.3350 - val_acc: 0.6120\n",
      "Epoch 17/30\n",
      "141/141 [==============================] - 15s 104ms/step - loss: 1.4782 - acc: 0.6807 - val_loss: 2.3067 - val_acc: 0.6174\n",
      "Epoch 18/30\n",
      "141/141 [==============================] - 15s 106ms/step - loss: 1.3816 - acc: 0.6950 - val_loss: 2.2737 - val_acc: 0.6200\n",
      "Epoch 19/30\n",
      "141/141 [==============================] - 20s 141ms/step - loss: 1.2853 - acc: 0.7120 - val_loss: 2.2513 - val_acc: 0.6256\n",
      "Epoch 20/30\n",
      "141/141 [==============================] - 22s 157ms/step - loss: 1.1985 - acc: 0.7257 - val_loss: 2.2314 - val_acc: 0.6298\n",
      "Epoch 21/30\n",
      "141/141 [==============================] - 23s 164ms/step - loss: 1.1108 - acc: 0.7430 - val_loss: 2.2115 - val_acc: 0.6376\n",
      "Epoch 22/30\n",
      "141/141 [==============================] - 21s 148ms/step - loss: 1.0342 - acc: 0.7596 - val_loss: 2.2001 - val_acc: 0.6490\n",
      "Epoch 23/30\n",
      "141/141 [==============================] - 19s 138ms/step - loss: 0.9516 - acc: 0.7754 - val_loss: 2.1737 - val_acc: 0.6430\n",
      "Epoch 24/30\n",
      "141/141 [==============================] - 16s 116ms/step - loss: 0.8792 - acc: 0.7943 - val_loss: 2.1477 - val_acc: 0.6506\n",
      "Epoch 25/30\n",
      "141/141 [==============================] - 15s 109ms/step - loss: 0.8088 - acc: 0.8083 - val_loss: 2.1479 - val_acc: 0.6506\n",
      "Epoch 26/30\n",
      "141/141 [==============================] - 16s 111ms/step - loss: 0.7444 - acc: 0.8261 - val_loss: 2.1503 - val_acc: 0.6520\n",
      "Epoch 27/30\n",
      "141/141 [==============================] - 18s 124ms/step - loss: 0.6851 - acc: 0.8390 - val_loss: 2.1349 - val_acc: 0.6536\n",
      "Epoch 28/30\n",
      "141/141 [==============================] - 15s 107ms/step - loss: 0.6306 - acc: 0.8522 - val_loss: 2.1243 - val_acc: 0.6632 loss: 0.63\n",
      "Epoch 29/30\n",
      "141/141 [==============================] - 13s 93ms/step - loss: 0.5786 - acc: 0.8653 - val_loss: 2.1196 - val_acc: 0.6642\n",
      "Epoch 30/30\n",
      "141/141 [==============================] - 13s 96ms/step - loss: 0.5262 - acc: 0.8773 - val_loss: 2.1282 - val_acc: 0.6604\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e1dd5d5a30>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "#filename = 'model.h5'\n",
    "#checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "model.fit(trainX, trainY, epochs=30, batch_size=64,  validation_data=(testX, testY))#, callbacks=[checkpoint])#, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "53449a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import argmax\n",
    "# asignar un número entero a una palabra\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    " \n",
    "# generar la secuencia de origen del objetivo\n",
    "def predict_sequence(model, tokenizer, source):\n",
    "    #print(f\"S {source}\")\n",
    "    prediction = model.predict(source, verbose=0)[0]\n",
    "    #print(f\"PREDICTION {prediction}\")\n",
    "    integers = [argmax(vector) for vector in prediction]\n",
    "    target = list()\n",
    "    for i in integers:\n",
    "        word = word_for_id(i, tokenizer)\n",
    "        #print(f\"WORD{word}\")\n",
    "        if word is None:\n",
    "            break\n",
    "        target.append(word)\n",
    "    return' '.join(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "8a10f635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[51  6 43  0  0  0  0  0  0  0]]\n",
      "[51  6 43  0  0  0  0  0  0  0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'turn it off'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_prediction_test=encode_sequences(ger_tokenizer, ger_length, ['Mach das aus.'])\n",
    "print(x_prediction_test)\n",
    "print(x_prediction_test[0])\n",
    "predict_sequence(model,eng_tokenizer,x_prediction_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
